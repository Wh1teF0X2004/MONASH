{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b834463",
   "metadata": {},
   "source": [
    "<div style= \"text-align: right\">\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: x-large;\">FIT3182 Big Data Management and Processing</p>\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: large;\">Assignment 2</p>\n",
    "    <p style= \"text-align: right\">Foo Kai Yan</p>\n",
    "    <p style= \"text-align: right\">kfoo0012@student.monash.edu<br><br><i>33085625<br><br><i>22<sup>th</sup> May 2024</i></p>\n",
    "<div>\n",
    "<hr style=\"border-color: black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a88376",
   "metadata": {},
   "source": [
    "## Student Statement\n",
    "The assignment was completed with the assistance of some code obtained from seminar/tutorial/lab/applied class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce768c0",
   "metadata": {},
   "source": [
    "### Installing PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5bc036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.8/site-packages (4.3.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from pymongo) (2.3.0)\n",
      "Requirement already satisfied: pygeohash in /opt/conda/lib/python3.8/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install pygeohash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c6d7f",
   "metadata": {},
   "source": [
    "### Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f69ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pygeohash as pgh\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf # spark\n",
    "from pyspark.streaming import StreamingContext # spark streaming\n",
    "from pyspark.sql.functions import col, split, element_at, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791d2db",
   "metadata": {},
   "source": [
    "### Check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07df9969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/student/ASSIGNMENT2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb931ae4",
   "metadata": {},
   "source": [
    "### Set os environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f01459",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f953f",
   "metadata": {},
   "source": [
    "### Set host ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89789e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hostip obtained using `ipconfig` command in command prompt\n",
    "hostip = \"10.192.45.141\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61252c",
   "metadata": {},
   "source": [
    "### Streaming Application Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38921b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check proximity based on geohash, precision 5\n",
    "def geohash_proximity_5(record, climate):\n",
    "    record_geohash = pgh.encode(record[\"longitude\"], record[\"latitude\"], precision = 5)\n",
    "    climate_geohash = pgh.encode(climate[0][\"longitude\"], climate[0][\"latitude\"], precision = 5)\n",
    "    return record_geohash == climate_geohash\n",
    "\n",
    "# Check proximity based on geohash, precision 3\n",
    "def geohash_proximity(record, climate):\n",
    "    record_geohash = pgh.encode(record[\"longitude\"], record[\"latitude\"], precision = 3)\n",
    "    climate_geohash = pgh.encode(climate[0][\"longitude\"], climate[0][\"latitude\"], precision = 3)\n",
    "    return record_geohash == climate_geohash\n",
    "\n",
    "def process_producer_data(batch_df, batch_id):\n",
    "    all_data = batch_df.collect()  # Returns all elements as an array \n",
    "    # Send stream data to be transformed & analysed\n",
    "    producer_data = [row.asDict() for row in all_data] \n",
    "    \n",
    "    # Initialize empty lists for aqua_hotspots_record, terra_hotspots_record, and an empty dictionary for climate_record\n",
    "    climate_record = []\n",
    "    aqua_hotspots_record = []\n",
    "    terra_hotspots_record = [] \n",
    "    \n",
    "    # Producer 1: Climate topic_name = \"Climate\", [\"producer_id\"] = \"producer1_climate\"\n",
    "    # Producer 2: AQUA topic_name = \"Hotspot_AQUA\", [\"producer_id\"] = \"producer2_hotspot_aqua\"\n",
    "    # Producer 3: TERRA topic_name = \"Hotspot_TERRA\", [\"producer_id\"] = \"producer3_hotspot_terra\"\n",
    "    \n",
    "    # For each item from the data batch from the Kafka stream \n",
    "    for eachdata in range(len(producer_data)): \n",
    "        # Check if 'value' key exists and is a bytearray\n",
    "        if 'value' in producer_data[eachdata] and isinstance(producer_data[eachdata]['value'], bytearray):\n",
    "            # Convert byte array to string and then to JSON\n",
    "            newdata = producer_data[eachdata]['value'].decode('utf-8')\n",
    "            produced_data = json.loads(newdata)\n",
    "            producer_data[eachdata] = produced_data\n",
    "            \n",
    "    for producer in producer_data:\n",
    "        producer_id = producer[\"producer_id\"]\n",
    "\n",
    "        # Sort data depending on the producer_id information\n",
    "        if producer_id == \"producer1_climate\":\n",
    "            climate_record.append(producer)\n",
    "            # print(\"climate record\", climate_record)\n",
    "        elif producer_id == \"producer2_hotspot_aqua\":\n",
    "            aqua_hotspots_record.append(producer)\n",
    "            # print(\"aqua_hotspots_record\", aqua_hotspots_record)\n",
    "        elif producer_id == \"producer3_hotspot_terra\":\n",
    "            terra_hotspots_record.append(producer)\n",
    "            # print(\"terra_hotspots_record\", terra_hotspots_record)\n",
    "                \n",
    "    # If there are no climate_record, skip processing this batch\n",
    "    # Check if the climate_record is empty, have 1 or more than 1 climate_record\n",
    "    if len(climate_record) == 1:\n",
    "        climate_record = climate_record\n",
    "    elif len(climate_record) > 1:\n",
    "        # If more than 1 then select the first climate_record only\n",
    "        climate_record = climate_record[0]\n",
    "    else:\n",
    "        print(\"No climate record present in this batch. Skipping...\")\n",
    "        return  # Exit the function\n",
    "\n",
    "    # Analyse hotspots data, find if any are close by & merge\n",
    "    new_hotspots_record = process_hotspots(aqua_hotspots_record, terra_hotspots_record, climate_record)\n",
    "    # Merge hotspots with climate depending if close & label if natural or other\n",
    "    new_climate_record = process_climate(climate_record, new_hotspots_record)\n",
    "    print(\"BACK TO MAIN FUNCTION\")\n",
    "    return addtodatabase(new_climate_record)\n",
    "\n",
    "def process_hotspots(aqua_hotspots_record, terra_hotspots_record, climate_record):\n",
    "    # Initialize empty array for hotspots records from both terra and aqua\n",
    "    hotspots = []\n",
    "    print(\"IN PROCESS HOTSPOTS\")\n",
    "    \n",
    "    # Process aqua_hotspots_record\n",
    "    for each_record in aqua_hotspots_record:\n",
    "        print(\"IN AQUA\")\n",
    "        if geohash_proximity(each_record, climate_record):\n",
    "            hotspots.append(each_record)\n",
    "            \n",
    "    # Process terra_hotspots_record\n",
    "    for each_record in terra_hotspots_record:\n",
    "        print(\"IN TERRA\")\n",
    "        if geohash_proximity(each_record, climate_record):\n",
    "            hotspots.append(each_record)\n",
    "            \n",
    "    return hotspots\n",
    "\n",
    "def process_climate(climate_record, hotspots_record):\n",
    "    # Initialize empty array for fire events records\n",
    "    print(\"IN PROCESS CLIMATE\")\n",
    "    fire_events = []\n",
    "    for each_record in hotspots_record:\n",
    "        fire_event = process_fire_event(climate_record, each_record)\n",
    "        if fire_event is not None:\n",
    "            fire_events.append(fire_event)\n",
    "    print(\"FIRE_EVENTS: \", fire_events)\n",
    "    # Add new key to climate_record and add the fire_events as the value to the key\n",
    "    # climate_record format: [{}] ; fire_events format: [{}]\n",
    "    new_key = \"fire_events\"\n",
    "    new_value = fire_events\n",
    "    \n",
    "    # Since the top-level structure is a list, access the first dictionary\n",
    "    if climate_record:  # Check if the list is not empty\n",
    "        climate_record[0][new_key] = new_value\n",
    "    else:\n",
    "        # If the list is empty, create a new dictionary with the new key\n",
    "        climate_record.append({new_key: new_value})\n",
    "    print(\"CLIMATE RECORD FROM PROCESS CLIMATE: \", climate_record)\n",
    "    return climate_record\n",
    "\n",
    "def process_fire_event(climate_record, hotspots_record):\n",
    "    print(\"IN PROCESS FIRE EVENT\")\n",
    "    # Initialize an empty dictionary to store fire event data\n",
    "    fire_happening = {}\n",
    "\n",
    "    # Check if the hotspot is geographically close to the climate data with a precision of 5\n",
    "    if geohash_proximity_5(hotspots_record, climate_record) is True:\n",
    "        print(\"hotspots_record: \", hotspots_record)\n",
    "        \n",
    "        # Get the created_time for Data Visualisation \n",
    "        fire_happening[\"created_time\"] = hotspots_record.get(\"created_time\")\n",
    "        fire_happening[\"created_hour\"] = hotspots_record.get(\"created_time\").split(':')[0]\n",
    "        \n",
    "        # Calculate the average surface temperature from both hotspot and climate data\n",
    "        average_temp = (hotspots_record.get(\"surface_temperature_celcius\") + hotspots_record.get(\"surface_temperature_celcius\")) / 2\n",
    "        fire_happening[\"average_surface_temp\"] = average_temp\n",
    "\n",
    "        # Calculate the average confidence level from both hotspot and climate data\n",
    "        average_confidence = (hotspots_record.get(\"confidence\") + hotspots_record.get(\"confidence\")) / 2\n",
    "        fire_happening[\"confidence\"] = average_confidence\n",
    "\n",
    "        # Get the air_temperature_celcius and GHI_w/m2 value\n",
    "        if climate_record:  # Check if the list is not empty\n",
    "            air_temp = climate_record[0].get(\"air_temperature_celcius\")\n",
    "            solar_irradiance = climate_record[0].get(\"GHI_w/m2\")\n",
    "        else:\n",
    "            air_temp = None\n",
    "            solar_irradiance = None\n",
    "        # Determine the cause of the fire based on climate conditions            \n",
    "        if air_temp > 20 and solar_irradiance > 180:\n",
    "            fire_happening[\"cause\"] = \"natural\"\n",
    "        else:\n",
    "            fire_happening[\"cause\"] = \"others\"\n",
    "        \n",
    "        print(\"FIRE HAPPENING: \", fire_happening)\n",
    "        # Return the dictionary containing fire event data\n",
    "        return fire_happening\n",
    "    # If the hotspot is not close to the climate data, return None\n",
    "    return None\n",
    "\n",
    "def addtodatabase(dictionary_in_array):\n",
    "    print(\"ADD TO DATABASE\")\n",
    "    # Only save data with contents\n",
    "    if len(dictionary_in_array) >= 1:\n",
    "\n",
    "        client = MongoClient(f'mongodb://{hostip}:27017/')\n",
    "\n",
    "        db = client.fit3182_assignment_db\n",
    "        climate_collection = db.climate_collection\n",
    "\n",
    "        # Insert climate data into database\n",
    "        inserting = climate_collection.insert_one(dictionary_in_array[0])\n",
    "        print(\"ADD SUCCESSFUL: \", inserting)\n",
    "\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d4acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of three Kafka topics separated by commas (Obtained from Producer1, 2 and 3)\n",
    "topic = \"Climate, Hotspot_AQUA, Hotspot_TERRA\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]') # local[*] means Spark will run locally using all available cores on the machine\n",
    "    .appName('Streaming Climate Data')\n",
    "    .getOrCreate() # Creates a SparkSession if it does not exist or returns the existing one\n",
    ")\n",
    "\n",
    "# This is basically connecting to kafka server\n",
    "topic_stream_df = (\n",
    "    spark.readStream.format('kafka') # 'kafka' because u are receiving from kafka\n",
    "    .option('kafka.bootstrap.servers', f'{hostip}:9092')\n",
    "    .option('subscribe', topic) # Subscribes to the Kafka topics listed in the topic variable\n",
    "    .load() # Loads the data stream from Kafka and returns it as a DataFrame\n",
    ")\n",
    "\n",
    "# Creates a new DataFrame data_sdf by selecting only the ‘value’ column from the topic_stream_df DataFrame\n",
    "# This ‘value’ column contains the actual message data obtained from Kafka\n",
    "data_sdf = topic_stream_df.select('value')\n",
    "\n",
    "db_writer = (\n",
    "    data_sdf\n",
    "    .writeStream # Indicates that the data in data_sdf will be written to an output sink\n",
    "    .outputMode('append') # Only new rows will be written to the output sink as they arrive\n",
    "    # collect data for 10 seconds\n",
    "    .trigger(processingTime='10 seconds')\n",
    "    # each batch of 10 seconds will run func\n",
    "    .foreachBatch(process_producer_data)) \n",
    "    # For each batch collected, the process_producer_data function will be called to process the data\n",
    "    # process_producer_data function is essentially the main function for the stream data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc79ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No climate record present in this batch. Skipping...\n",
      "No climate record present in this batch. Skipping...\n",
      "IN PROCESS HOTSPOTS\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN TERRA\n",
      "IN TERRA\n",
      "IN PROCESS CLIMATE\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "FIRE_EVENTS:  []\n",
      "CLIMATE RECORD FROM PROCESS CLIMATE:  [{'latitude': -37.478, 'longitude': 148.117, 'air_temperature_celcius': 11.0, 'relative_humidity': 43.9, 'windspeed_knots': 11.2, 'max_wind_speed': 16.9, 'GHI_w/m2': 99.0, 'precipitation_flag': 'G', 'precipitation': 0.12, 'latest_date': '2024-01-10T00:00:00', 'producer_id': 'producer1_climate', 'fire_events': []}]\n",
      "BACK TO MAIN FUNCTION\n",
      "ADD TO DATABASE\n",
      "ADD SUCCESSFUL:  <pymongo.results.InsertOneResult object at 0x7f0330654370>\n",
      "IN PROCESS HOTSPOTS\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN TERRA\n",
      "IN TERRA\n",
      "IN PROCESS CLIMATE\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "hotspots_record:  {'latitude': -37.61, 'longitude': 149.279, 'confidence': 69.0, 'surface_temperature_celcius': 48.0, 'created_time': '05:37:36', 'producer_id': 'producer2_hotspot_aqua'}\n",
      "FIRE HAPPENING:  {'created_time': '05:37:36', 'created_hour': '05', 'average_surface_temp': 48.0, 'confidence': 69.0, 'cause': 'others'}\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "FIRE_EVENTS:  [{'created_time': '05:37:36', 'created_hour': '05', 'average_surface_temp': 48.0, 'confidence': 69.0, 'cause': 'others'}]\n",
      "CLIMATE RECORD FROM PROCESS CLIMATE:  [{'latitude': -37.611, 'longitude': 149.277, 'air_temperature_celcius': 18.0, 'relative_humidity': 49.4, 'windspeed_knots': 9.3, 'max_wind_speed': 13.0, 'GHI_w/m2': 155.0, 'precipitation_flag': 'I', 'precipitation': 0.0, 'latest_date': '2024-01-11T00:00:00', 'producer_id': 'producer1_climate', 'fire_events': [{'created_time': '05:37:36', 'created_hour': '05', 'average_surface_temp': 48.0, 'confidence': 69.0, 'cause': 'others'}]}]\n",
      "BACK TO MAIN FUNCTION\n",
      "ADD TO DATABASE\n",
      "ADD SUCCESSFUL:  <pymongo.results.InsertOneResult object at 0x7f03305ba3d0>\n",
      "IN PROCESS HOTSPOTS\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN TERRA\n",
      "IN TERRA\n",
      "IN TERRA\n",
      "IN PROCESS CLIMATE\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "FIRE_EVENTS:  []\n",
      "CLIMATE RECORD FROM PROCESS CLIMATE:  [{'latitude': -37.296, 'longitude': 144.386, 'air_temperature_celcius': 11.0, 'relative_humidity': 40.8, 'windspeed_knots': 12.2, 'max_wind_speed': 20.0, 'GHI_w/m2': 102.0, 'precipitation_flag': 'G', 'precipitation': 0.24, 'latest_date': '2024-01-12T00:00:00', 'producer_id': 'producer1_climate', 'fire_events': []}]\n",
      "BACK TO MAIN FUNCTION\n",
      "ADD TO DATABASE\n",
      "ADD SUCCESSFUL:  <pymongo.results.InsertOneResult object at 0x7f03305ba940>\n",
      "IN PROCESS HOTSPOTS\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN TERRA\n",
      "IN TERRA\n",
      "IN PROCESS CLIMATE\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "FIRE_EVENTS:  []\n",
      "CLIMATE RECORD FROM PROCESS CLIMATE:  [{'latitude': -38.527, 'longitude': 143.493, 'air_temperature_celcius': 19.0, 'relative_humidity': 51.5, 'windspeed_knots': 10.2, 'max_wind_speed': 20.0, 'GHI_w/m2': 161.0, 'precipitation_flag': 'I', 'precipitation': 0.0, 'latest_date': '2024-01-13T00:00:00', 'producer_id': 'producer1_climate', 'fire_events': []}]\n",
      "BACK TO MAIN FUNCTION\n",
      "ADD TO DATABASE\n",
      "ADD SUCCESSFUL:  <pymongo.results.InsertOneResult object at 0x7f03305ba160>\n",
      "IN PROCESS HOTSPOTS\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN AQUA\n",
      "IN TERRA\n",
      "IN TERRA\n",
      "IN TERRA\n",
      "IN PROCESS CLIMATE\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "IN PROCESS FIRE EVENT\n",
      "FIRE_EVENTS:  []\n",
      "CLIMATE RECORD FROM PROCESS CLIMATE:  [{'latitude': -37.926, 'longitude': 144.0898, 'air_temperature_celcius': 15.0, 'relative_humidity': 51.0, 'windspeed_knots': 9.0, 'max_wind_speed': 13.0, 'GHI_w/m2': 128.0, 'precipitation_flag': 'G', 'precipitation': 1.26, 'latest_date': '2024-01-14T00:00:00', 'producer_id': 'producer1_climate', 'fire_events': []}]\n",
      "BACK TO MAIN FUNCTION\n",
      "ADD TO DATABASE\n",
      "ADD SUCCESSFUL:  <pymongo.results.InsertOneResult object at 0x7f03305bac10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopping query.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code section manage the lifecycle of a streaming query in a controlled manner\n",
    "It allows for a graceful shutdown when the user decides to interrupt the process\n",
    "'''\n",
    "try:\n",
    "    query = db_writer.start() # Starts the streaming query\n",
    "    query.awaitTermination() # Waits for the streaming query to finish\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.') # Gracefully handle a user’s request to interrupt the program\n",
    "finally:\n",
    "    query.stop() # Stops the streaming query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d0d2a",
   "metadata": {},
   "source": [
    "### Double Check data is added to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85383d71",
   "metadata": {},
   "source": [
    "client = MongoClient(f'mongodb://{hostip}:27017/')\n",
    "db = client.fit3182_assignment_db\n",
    "climate_collection = db.climate_collection\n",
    "cursor = db.climate_collection.find({})\n",
    "\n",
    "for document in cursor:\n",
    "    pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b31f56",
   "metadata": {},
   "source": [
    "If there are fire events:\n",
    "```\n",
    "{'GHI_w/m2': 152.0,\n",
    " '_id': ObjectId('664db85c06058a4bb12f4f43'),\n",
    " 'air_temperature_celcius': 18.0,\n",
    " 'fire_events': [{'average_surface_temp': 48.0,\n",
    "                  'cause': 'others',\n",
    "                  'confidence': 74.0,\n",
    "                  'created_hour': '17',\n",
    "                  'created_time': '17:15:55'}],\n",
    " 'latest_date': '2027-10-20T00:00:00',\n",
    " 'latitude': -36.2032,\n",
    " 'longitude': 145.3025,\n",
    " 'max_wind_speed': 14.0,\n",
    " 'precipitation': 0.0,\n",
    " 'precipitation_flag': 'I',\n",
    " 'producer_id': 'producer1_climate',\n",
    " 'relative_humidity': 52.0,\n",
    " 'windspeed_knots': 7.1}\n",
    "```\n",
    "\n",
    "If there are no fire events:\n",
    "```\n",
    "{'GHI_w/m2': 102.0,\n",
    " '_id': ObjectId('664d92c887f00ff1a1c1bc4d'),\n",
    " 'air_temperature_celcius': 11.0,\n",
    " 'fire_events': [],\n",
    " 'latest_date': '2025-03-03T00:00:00',\n",
    " 'latitude': -37.8088,\n",
    " 'longitude': 142.2291,\n",
    " 'max_wind_speed': 8.9,\n",
    " 'precipitation': 0.0,\n",
    " 'precipitation_flag': 'G',\n",
    " 'producer_id': 'producer1_climate',\n",
    " 'relative_humidity': 40.8,\n",
    " 'windspeed_knots': 6.4}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
