{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bad6a4",
   "metadata": {},
   "source": [
    "<div style= \"text-align: right\">\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: x-large;\">FIT3182 Big Data Management and Processing</p>\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: large;\">Assignment 2</p>\n",
    "    <p style= \"text-align: right\">Foo Kai Yan</p>\n",
    "    <p style= \"text-align: right\">kfoo0012@student.monash.edu<br><br><i>33085625<br><br><i>18<sup>th</sup> May 2024</i></p>\n",
    "<div>\n",
    "<hr style=\"border-color: black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28f319",
   "metadata": {},
   "source": [
    "## Student Statement\n",
    "The assignment was completed with the assistance of some code obtained from seminar/tutorial/lab/applied class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fc701",
   "metadata": {},
   "source": [
    "### Installing PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae485fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.8/site-packages (4.3.3)\r\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from pymongo) (2.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98215dc7",
   "metadata": {},
   "source": [
    "### Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8606e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "from kafka3 import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9f3f8",
   "metadata": {},
   "source": [
    "### Check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285c6915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/student/ASSIGNMENT2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6d796",
   "metadata": {},
   "source": [
    "## Producer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9520d665",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records...\n",
      "Message published successfully. Data: {'latitude': -37.436, 'longitude': 148.088, 'air_temperature_celcius': 15.0, 'relative_humidity': 41.5, 'windspeed_knots': 17.0, 'max_wind_speed': 28.9, 'GHI_w/m2': 138.0, 'precipitation_flag': 'I', 'precipitation': 0.0, 'latest_date': '2024-01-02T00:00:00', 'producer_id': 'producer1_climate'}\n",
      "Message published successfully. Data: {'latitude': -36.098, 'longitude': 143.735, 'air_temperature_celcius': 17.0, 'relative_humidity': 58.1, 'windspeed_knots': 11.7, 'max_wind_speed': 19.0, 'GHI_w/m2': 136.0, 'precipitation_flag': 'G', 'precipitation': 0.04, 'latest_date': '2024-01-03T00:00:00', 'producer_id': 'producer1_climate'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m\n\u001b[1;32m     91\u001b[0m selected_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducer1_climate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m publish_message(producer, topic_name, selected_data) \u001b[38;5;66;03m# Publish message\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hostip obtained using `ipconfig` command in command prompt\n",
    "hostip = \"192.168.68.58\"\n",
    "\n",
    "# By opening the climate_streaming.csv, the column names is already known so one by one copy paste method was used to get the data from the climate_streaming.csv\n",
    "def readClimateStreamingCSV():\n",
    "    '''\n",
    "    readClimateStreamingCSV function:\n",
    "    - reads data from 'climate_streaming.csv' using pandas\n",
    "    - returns an array\n",
    "    '''\n",
    "    # Read the CSV file into a DataFrame\n",
    "    climate_streaming_data = pd.read_csv('climate_streaming.csv')\n",
    "    climate_streaming_data_array = []\n",
    "    \n",
    "    # Iterate through each row in the DataFrame using the index\n",
    "    for index in range(len(climate_streaming_data)):\n",
    "        # Access the row by its index\n",
    "        row = climate_streaming_data.iloc[index]\n",
    "        \n",
    "        # Create a dictionary for each row's data\n",
    "        climate_streaming_data_point = {\n",
    "            \"latitude\": float(row[\"latitude\"]),\n",
    "            \"longitude\": float(row[\"longitude\"]),\n",
    "            \"air_temperature_celcius\": float(row[\"air_temperature_celcius\"]),\n",
    "            \"relative_humidity\": float(row[\"relative_humidity\"]),\n",
    "            \"windspeed_knots\": float(row[\"windspeed_knots\"]),\n",
    "            \"max_wind_speed\": float(row[\"max_wind_speed\"]),\n",
    "            \"GHI_w/m2\": float(row[\"GHI_w/m2\"])\n",
    "        }\n",
    "        \n",
    "        # Process the precipitation data\n",
    "        precipitation = str(row[\"precipitation \"]).strip()  # Remove any leading/trailing whitespace\n",
    "        if precipitation:\n",
    "            # Split precipitation type and amount\n",
    "            climate_streaming_data_point['precipitation_flag'] = precipitation[-1]\n",
    "            climate_streaming_data_point[\"precipitation\"] = float(precipitation[:-1])\n",
    "        else:\n",
    "            # Handle missing or empty precipitation data\n",
    "            climate_streaming_data_point['precipitation_flag'] = None\n",
    "            climate_streaming_data_point[\"precipitation\"] = None\n",
    "        \n",
    "        # Append the dictionary to the array\n",
    "        climate_streaming_data_array.append(climate_streaming_data_point)\n",
    "\n",
    "    return climate_streaming_data_array\n",
    "\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    '''\n",
    "    publish_message function: \n",
    "    - takes a Kafka producer instance, a topic name, and data\n",
    "    - then attempts to send the data to the specified Kafka topic. \n",
    "    - If successful, it prints a confirmation message; otherwise, it prints an error message.\n",
    "    '''\n",
    "    try:\n",
    "        producer_instance.send(topic_name, value=data)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    '''\n",
    "    connect_kafka_producer function: \n",
    "    - attempts to connect to a Kafka producer using the host IP initialised beforehand and port 9092\n",
    "    - returning the producer instance or None if unsuccessful.\n",
    "    '''\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('utf-8'),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = readClimateStreamingCSV()\n",
    "    topic_name = \"Climate\"\n",
    "    producer = connect_kafka_producer()\n",
    "    latest_date = dt.datetime(2024, 1, 1) # Last date from historic CSV is 1/1/2024\n",
    "    print(\"Publishing records...\")\n",
    "\n",
    "    while True:\n",
    "        random_number = random.randrange(0, len(data))\n",
    "        selected_data = data[random_number] # Pick a random climate data point\n",
    "        latest_date += dt.timedelta(days=1) # Increase date from previous date\n",
    "        selected_data[\"latest_date\"] = latest_date.isoformat() # Set date to string format (to be stored in JSON)\n",
    "        selected_data[\"producer_id\"] = \"producer1_climate\"\n",
    "\n",
    "        publish_message(producer, topic_name, selected_data) # Publish message\n",
    "\n",
    "        sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce8f93",
   "metadata": {},
   "source": [
    "The code above read climate data from 'climate_streaming.csv' and publish it to a Kafka topic. \n",
    "- The function `readClimateStreamingCSV` reads data from 'climate_streaming.csv' using pandas.\n",
    "- Each row is processed to extract climate attributes, and stores them in a list of dictionaries. \n",
    "- `publish_message` function takes a Kafka producer instance, a topic name, and data, then attempts to send the data to the specified Kafka topic. If successful, it prints a confirmation message; otherwise, it prints an error message. \n",
    "- `connect_kafka_producer` function attempts to connect to a Kafka producer using the host IP initialised beforehand and port 9092, returning the producer instance or `None` if unsuccessful.\n",
    "\n",
    "In the main execution block, \n",
    "```\n",
    "if __name__ == '__main__':\n",
    "```\n",
    "- Climate streaming data is read with `readClimateStreamingCSV`\n",
    "- Kafka producer is connected with `connect_kafka_producer`\n",
    "- A date variable with the last date from the historic CSV file is initialised with `latest_date`\n",
    "- An infinite loop is entered where it selects a random data point from the climate data, increments the date by one day, formats the date for JSON storage, adds a producer ID, and publishes the message to the Kafka topic `Climate`. \n",
    "    - The loop pauses for 10 seconds before repeating the process, effectively streaming the climate data to the Kafka topic at regular intervals. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
