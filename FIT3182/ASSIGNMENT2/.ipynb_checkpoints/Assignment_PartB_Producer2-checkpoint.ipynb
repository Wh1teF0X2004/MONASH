{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a42dd42",
   "metadata": {},
   "source": [
    "<div style= \"text-align: right\">\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: x-large;\">FIT3182 Big Data Management and Processing</p>\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: large;\">Assignment 2</p>\n",
    "    <p style= \"text-align: right\">Foo Kai Yan</p>\n",
    "    <p style= \"text-align: right\">kfoo0012@student.monash.edu<br><br><i>33085625<br><br><i>18<sup>th</sup> May 2024</i></p>\n",
    "<div>\n",
    "<hr style=\"border-color: black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb349226",
   "metadata": {},
   "source": [
    "## Student Statement\n",
    "The assignment was completed with the assistance of some code obtained from seminar/tutorial/lab/applied class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c55f2",
   "metadata": {},
   "source": [
    "### Installing PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f6edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.8/site-packages (4.3.3)\r\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from pymongo) (2.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffea150",
   "metadata": {},
   "source": [
    "### Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf3a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "from kafka3 import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba8f71",
   "metadata": {},
   "source": [
    "### Check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f2ed7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/student/ASSIGNMENT2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9bfd4",
   "metadata": {},
   "source": [
    "## Producer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5a0dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message published successfully. Data: {'latitude': -37.9408, 'longitude': 146.0611, 'confidence': 58.0, 'surface_temperature_celcius': 47.0, 'created_time': '10:29:37', 'producer_id': 'producer2_hotspot_aqua'}\n",
      "Message published successfully. Data: {'latitude': -37.34, 'longitude': 143.123, 'confidence': 81.0, 'surface_temperature_celcius': 56.0, 'created_time': '10:29:37', 'producer_id': 'producer2_hotspot_aqua'}\n",
      "Message published successfully. Data: {'latitude': -37.4492, 'longitude': 148.2438, 'confidence': 100.0, 'surface_temperature_celcius': 47.0, 'created_time': '10:29:37', 'producer_id': 'producer2_hotspot_aqua'}\n",
      "Message published successfully. Data: {'latitude': -37.8777, 'longitude': 143.371, 'confidence': 81.0, 'surface_temperature_celcius': 54.0, 'created_time': '10:29:37', 'producer_id': 'producer2_hotspot_aqua'}\n",
      "Message published successfully. Data: {'latitude': -36.0994, 'longitude': 143.4156, 'confidence': 82.0, 'surface_temperature_celcius': 62.0, 'created_time': '10:29:37', 'producer_id': 'producer2_hotspot_aqua'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 89\u001b[0m\n\u001b[1;32m     85\u001b[0m selected_data_point[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducer2_hotspot_aqua\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m publish_message(producer, topic_name, selected_data_point)\n\u001b[0;32m---> 89\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_integer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hostip obtained using `ipconfig` command in command prompt\n",
    "hostip = \"192.168.68.58\"\n",
    "\n",
    "# By opening the hotspot_AQUA_streaming.csv, the column names is already known so one by one copy paste method was used to get the data from the hotspot_AQUA_streaming.csv\n",
    "def readHotpotStreamingCSV():\n",
    "    '''\n",
    "    readHotpotStreamingCSV function:\n",
    "    - reads data from 'hotspot_AQUA_streaming.csv' using pandas\n",
    "    - return an array\n",
    "    '''\n",
    "    # Read the CSV file into a DataFrame\n",
    "    hotspot_AQUA_streaming_data = pd.read_csv(\"hotspot_AQUA_streaming.csv\")\n",
    "    hotspot_AQUA_streaming_data_array = []\n",
    "    \n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index in range(len(hotspot_AQUA_streaming_data)):\n",
    "        # Access the row by its index\n",
    "        row = hotspot_AQUA_streaming_data.iloc[index]\n",
    "        \n",
    "        # Create a dictionary for each row's data\n",
    "        hotspot_AQUA_streaming_data_point = {\n",
    "            \"latitude\": float(row[\"latitude\"]),\n",
    "            \"longitude\": float(row[\"longitude\"]),\n",
    "            \"confidence\": float(row[\"confidence\"]),\n",
    "            \"surface_temperature_celcius\": float(row[\"surface_temperature_celcius\"])\n",
    "        }\n",
    "        \n",
    "        # Append the dictionary to the array\n",
    "        hotspot_AQUA_streaming_data_array.append(hotspot_AQUA_streaming_data_point)\n",
    "\n",
    "    return hotspot_AQUA_streaming_data_array\n",
    "\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    '''\n",
    "    publish_message function: \n",
    "    - takes a Kafka producer instance, a topic name, and data\n",
    "    - then attempts to send the data to the specified Kafka topic. \n",
    "    - If successful, it prints a confirmation message; otherwise, it prints an error message.\n",
    "    '''\n",
    "    try:\n",
    "        producer_instance.send(topic_name, value=data)\n",
    "        producer_instance.flush()\n",
    "        print(\"Message published successfully. Data: \" + str(data))\n",
    "    except Exception as ex:\n",
    "        print(\"Exception in publishing message.\")\n",
    "        print(str(ex))\n",
    "\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    '''\n",
    "    connect_kafka_producer function: \n",
    "    - attempts to connect to a Kafka producer using the host IP initialised beforehand and port 9092\n",
    "    - returning the producer instance or None if unsuccessful.\n",
    "    '''\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode(\"ascii\"),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print(\"Exception while connecting Kafka.\")\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data = readHotpotStreamingCSV() # Read all data from CSV at one time.\n",
    "    topic_name = \"Hotspot_AQUA\"\n",
    "    producer = connect_kafka_producer()\n",
    "    \n",
    "    random_integer = random.randint(1,10) # min is 1, max is 10\n",
    "    \n",
    "    random_date = str(dt.datetime.now()) # Random date (I choose tdy as the 'random date')\n",
    "    created_date = dt.datetime.strptime(random_date, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    data_length = len(data)\n",
    "    for alldata in range(data_length):\n",
    "        # Choose random aqua data point\n",
    "        random_index_number = random.randrange(0, data_length)\n",
    "        selected_data_point = data[random_index_number]\n",
    "        # random_integer max is 10 cus since 10s = 24 so the hr would be 24/10*random_integer\n",
    "        the_hours = (24/10)*random_integer\n",
    "        new_date = created_date + dt.timedelta(hours = the_hours)\n",
    "        selected_data_point[\"created_time\"] = new_date.strftime(\"%H:%M:%S\") # Formating the time\n",
    "        selected_data_point[\"producer_id\"] = \"producer2_hotspot_aqua\"\n",
    "\n",
    "        publish_message(producer, topic_name, selected_data_point)\n",
    "\n",
    "        sleep(random_integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351b4e2",
   "metadata": {},
   "source": [
    "The code above read climate data from 'hotspot_AQUA_streaming.csv' and publish it to a Kafka topic. \n",
    "- The function `readHotpotStreamingCSV` reads data from 'hotspot_AQUA_streaming.csv' using pandas.\n",
    "- Each row is processed to extract hotspot AQUA attributes, and stores them in a list of dictionaries. \n",
    "- `publish_message` function takes a Kafka producer instance, a topic name, and data, then attempts to send the data to the specified Kafka topic. If successful, it prints a confirmation message; otherwise, it prints an error message. \n",
    "- `connect_kafka_producer` function attempts to connect to a Kafka producer using the host IP initialised beforehand and port 9092, returning the producer instance or `None` if unsuccessful.\n",
    "\n",
    "In the main execution block, \n",
    "```\n",
    "if __name__ == '__main__':\n",
    "```\n",
    "- Hotspot streaming data is read with `readHotpotStreamingCSV`\n",
    "- Kafka producer is connected with `connect_kafka_producer`\n",
    "- `random_integer = random.randint(1,10)` generates a random integer between 1 and 10\n",
    "- `random_date = str(dt.datetime.now())` gets the current date and time as a string which will be converted back into a datetime object in `created_date`\n",
    "- An for loop is entered where it selects a random data point from the Hotspot data, formats the date for JSON storage, adds a `producer_id`, and publishes the message to the Kafka topic `Hotspot_AQUA`. \n",
    "    - `the_hours = (24/10)*random_integer` calculates a new time offset based on the random integer\n",
    "    - `new_date = created_date + dt.timedelta(hours = the_hours)` adds the time offset to the created_date\n",
    "    - The loop pauses for n seconds before repeating the process where n is random_integer between 1 and 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
