{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5VW0TzHdR_"
   },
   "source": [
    "# FIT3182 - Assignment 1 - Questions Workbook\n",
    "\n",
    "This assignment consists of **four questions** that are, taken together, worth 10% of the final marks. **Please treat this Assignment as a take-home test. This is an individual assignment and you should complete the questions on your own**. You should not attempt to post questions on EdForum seeking solutions to the answers. If you require clarification on the Assignment questions, you can post a private post on EdForum or seek consultation from the tutors.\n",
    "\n",
    "1. The first question is related to **Parallel Search Algorithms (2 Mark)**.\n",
    "2. The second question is related to **Parallel Join Algorithms (4 Marks)**.\n",
    "3. The third question is related to **Parallel Sort Algorithms (2 Marks)**.\n",
    "4. The fourth question is related to **Parallel GroupBy Algorithms (2 Marks)**.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3. Answer all questions inside this Jupyter Notebook. Please use the provided Docker to load the Jupyter Notebook.\n",
    "- Read the instructions, skeleton code, and comments carefully.\n",
    "- There are **code blocks that you need to complete** yourself as a part of the assignment. \n",
    "- You are also required to **answer the questions below**. \n",
    "- **Comment each line of code properly such that the tutor can easily understand what you are trying to do in the code. Marks may be deducted for insufficient or unclear comments.**\n",
    "- Once completed, please rename the Jupyter Notebook to include your Student ID at the beginning of the filename (e.g., 12345678_FIT3182_Assignment_1.ipynb). Submit this Jupyter Notebook into the Assignment 1 submission link in Moodle. Please refer to the Assignment 1 submission link (or page) in Moodle for information about the submission due date.\n",
    "\n",
    "**Please include your details as follows:**\n",
    "- Student ID: 33085625\n",
    "- Last Name: Foo\n",
    "- First (or Preferred) Name: Kai Yan\n",
    "- Monash Student Email: kfoo0012@student.monash.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Statement\n",
    "The assignment was completed with the assistance of some code from tutorial/lab/applied class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRWxd1YrHdSB"
   },
   "source": [
    "## Dataset\n",
    "For this test, we will use the following tables R and S to write solutions to three parallel algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImhBOqgmHdSC"
   },
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "R = [('Adele',8),('Bob',22),('Clement',16),('Dave',23),('Ed',11),\n",
    "     ('Fung',25),('Goel',3),('Harry',17),('Irene',14),('Joanna',2),\n",
    "     ('Kelly',6),('Lim',20),('Meng',1),('Noor',5),('Omar',19)]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "S = [('Arts',8),('Business',15),('CompSc',2),('Dance',12),('Engineering',7),\n",
    "     ('Finance',21),('Geology',10),('Health',11),('IT',18)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7W3uUvJLErBY",
    "tags": []
   },
   "source": [
    "## 1. Parallel Searching Algorithm (2 marks)\n",
    "**This section consist of 4 sub-questions.**\n",
    "\n",
    "In this task, you will build a **parallel search algorithm for range selection (continuous)** for a given query.\n",
    "\n",
    " **Implement a parallel search algorithm** that uses local linear search (i.e., `linear_search()`) and is able to work with the hash partitioning method (i.e., `h_partition()`).\n",
    " **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvTYiOkyFJp8"
   },
   "outputs": [],
   "source": [
    "# Linear search function\n",
    "def linear_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform linear search on data for the given key.\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list OR numpy array)\n",
    "    key -- an query attribute (number)\n",
    "\n",
    "    Return:\n",
    "    result --  list of matched records such as [('Adele', 8)]\n",
    "                or the position of searched record\n",
    "                whichever is more appropriate for your code\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    match_record = None\n",
    "    position = -1 # not found position\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    for record in data:\n",
    "        # Because data is in tuple form so we extract data from tuple with this\n",
    "        name, number = record\n",
    "        if number == key:  # If number is matched with key\n",
    "            match_record = (name, number)\n",
    "            position = data.index(record)\n",
    "            # Add the matched record as a tuple in a list\n",
    "            matches.append(match_record)\n",
    "            break\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1555295423504,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "ruGmDOnfmPWc",
    "outputId": "494f8520-91c4-470d-e640-0bc5a6e184f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lim', 20)]\n"
     ]
    }
   ],
   "source": [
    "results = linear_search(R, 20)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hash Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "An_0xFW2FQvs"
   },
   "outputs": [],
   "source": [
    "# Define a simple hash function.\n",
    "def s_hash(x, n):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "\n",
    "    Arguments:\n",
    "    x -- an input record attribute (int)\n",
    "    n -- the number of processors (int)\n",
    "\n",
    "    Return:\n",
    "    result -- the hash value of x\n",
    "    \"\"\"\n",
    "    return x % n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMLZCK2BFYeF"
   },
   "outputs": [],
   "source": [
    "# Hash data partitioning function. \n",
    "# We will use the s_hash function defined above\n",
    "def h_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform hash data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list)\n",
    "    n -- the number of processors (int)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    partitions = {}\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    for record in data:\n",
    "        # Because data is in tuple form so we extract data from tuple with this\n",
    "        name, number = record\n",
    "        h = s_hash(number, n)  # Get the hash key of the input\n",
    "        if h in partitions:\n",
    "            partitions[h].append((name, number))\n",
    "        else:\n",
    "            partitions[h] = [(name, number)]\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1555295448743,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "Ako86IBFpqwj",
    "outputId": "dd8f1dc8-a73f-4455-f059-58cc0e79a01f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('Fung', 25), ('Lim', 20), ('Noor', 5)],\n",
      " 1: [('Clement', 16), ('Ed', 11), ('Kelly', 6), ('Meng', 1)],\n",
      " 2: [('Bob', 22), ('Harry', 17), ('Joanna', 2)],\n",
      " 3: [('Adele', 8), ('Dave', 23), ('Goel', 3)],\n",
      " 4: [('Irene', 14), ('Omar', 19)]}\n"
     ]
    }
   ],
   "source": [
    "partitions = h_partition(R, 5)\n",
    "pprint(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parallel Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlVKTCO-FkV9"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def collect_result(results, result):\n",
    "    # Callback for when linear_search returns a result.\n",
    "    # The results list is modified only by the main process, not the pool workers.\n",
    "    results.append(result)\n",
    "\n",
    "# Parallel searching algorithm for range selection\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    \"\"\"\n",
    "    Perform parallel search for range selection on data for the given key.\n",
    "\n",
    "    Arguments:\n",
    "    data -- the input dataset (list)\n",
    "    query_range -- a tuple with inclusive range end-points (e.g. [30, 50] means the interval 30-50)\n",
    "    n_processor -- the number of parallel processors (int)\n",
    "    \n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "    \n",
    "    pool = Pool(processes=n_processor)\n",
    "    \n",
    "    ### START CODE HERE ###    \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Perform data partitioning first\n",
    "    data_partitioning = h_partition(data, n_processor)\n",
    "    \n",
    "    for query in range(query_range[0], query_range[1], 1):\n",
    "        # Each element in data_partitioning has a pair (hash key: records)\n",
    "        query_hash = s_hash(query, n_processor)\n",
    "        partitioned_data = list(data_partitioning[query_hash])\n",
    "        result = pool.apply(linear_search, [partitioned_data, query])\n",
    "        collect_result(results, result)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1555295758812,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "vgfN1IcyFxaG",
    "outputId": "5fa6eb31-b14b-441e-fdad-a6767369df34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Noor', 5)],\n",
      " [('Kelly', 6)],\n",
      " [],\n",
      " [('Adele', 8)],\n",
      " [],\n",
      " [],\n",
      " [('Ed', 11)],\n",
      " [],\n",
      " [],\n",
      " [('Irene', 14)]]\n"
     ]
    }
   ],
   "source": [
    "n_processor = 3\n",
    "results = parallel_search_range(R, [5, 15], n_processor)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Parallel Search Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly answer the following.**\n",
    "\n",
    "1. How would the parallel search you implemented need to be changed if we switched to round-robin partition for the data? Provide a small snippet of code with the modified search loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers**:\n",
    "First I would have specifically implement the round-robin partition in another function and perform the round-robin partition for the data then save it in data_partitioning variable then only perform parallel search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_robin_partition(data, n_processor):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(n_processor):\n",
    "        result.append([])\n",
    "        \n",
    "    # For each bin, perform the following\n",
    "    for name, number in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (name % n_processor)\n",
    "        result[index_bin].append(number)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def round_robin_partition_parallel_search(data, query_range, n_processor):\n",
    "    pool = Pool(processes=n_processor) \n",
    "    results = []\n",
    "    \n",
    "    # Perform data partitioning first\n",
    "    data_partitioning = round_robin_partition(data, n_processor)\n",
    "    \n",
    "    for query in range(query_range[0], query_range[1], 1):\n",
    "        # Each element in data_partitioning has a pair (hash key: records)\n",
    "        query_hash = s_hash(query, n_processor)\n",
    "        partitioned_data = list(data_partitioning[query_hash])\n",
    "        result = pool.apply(linear_search, [partitioned_data, query])\n",
    "        collect_result(results, result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Irene', 14)]\n"
     ]
    }
   ],
   "source": [
    "n_processor = 3\n",
    "results = round_robin_partition_parallel_search(R, [12, 15], n_processor)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf29MfmUGWV1"
   },
   "source": [
    "## 2. Parallel Join Algorithm (4 marks)\n",
    "**This section consist of 5 sub-questions.**\n",
    "\n",
    "In this task, you will implement a **disjoint-partitioning based parallel join algorithm**. This algorithm consist of two stages.\n",
    "1. Data disjoint-partitioning across processors.\n",
    "1. Local join in each processor.\n",
    "\n",
    "As a data partitioning method, use the range partitioning algorithm  (i.e. `range_partition( )`). Assume that we have **3 parallel processors**, processor 1 will get records with join attribute value between 1 and 9, processor 2 between 10 and 19, and processor 3 between 20 and 29. Note that both tables R and S need to be partitioned using the same ranges described earlier.\n",
    "\n",
    "As a joining technique, use the hash based join algorithm (i.e., `HB_join( )`).  **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Range Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_BwmzrmHaTN"
   },
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list)\n",
    "    range_indices -- the range end-points (e.g., [5, 10, 15] means\n",
    "                        4 intervals: x < 5, 5 <= x < 10, 10 <= x < 15, 15 <= x)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    ### START CODE HERE ###  \n",
    "    \n",
    "    # First, we sort the dataset according their values  \n",
    "    sort_by_value = sorted(data, key=lambda x: x[1])\n",
    "\n",
    "    # Calculate the number of bins\n",
    "    bins_number = len(range_indices) \n",
    "\n",
    "    # For each bin, perform the following\n",
    "    for i in range(bins_number): \n",
    "        # Find elements to be belonging to each range\n",
    "        partitioned_list = [x for x in sort_by_value if x[1] < range_indices[i]] \n",
    "        # Add the partitioned list to the result\n",
    "        result.append(partitioned_list) \n",
    "        # Find the last element in the previous partition\n",
    "        last_element = partitioned_list[len(partitioned_list)-1]\n",
    "        # Find the index of of the last element\n",
    "        last = sort_by_value.index(last_element)\n",
    "        # Remove the partitioned list from the dataset\n",
    "        sort_by_value = sort_by_value[int(last)+1:] \n",
    "\n",
    "        # Append the last remaining data list\n",
    "    result.append([x for x in sort_by_value if x[1] >= range_indices[bins_number-1]]) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4eA2G_7vafZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Meng', 1),\n",
      "  ('Joanna', 2),\n",
      "  ('Goel', 3),\n",
      "  ('Noor', 5),\n",
      "  ('Kelly', 6),\n",
      "  ('Adele', 8)],\n",
      " [('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19)],\n",
      " [('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(range_partition(R, [10, 20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kONrahsIMmD"
   },
   "outputs": [],
   "source": [
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEpFbToJIPlr"
   },
   "outputs": [],
   "source": [
    "def HB_join(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute (in second position) in the input tables T1 & T2.\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    dic = {} # We will use a dictionary\n",
    "    \n",
    "    # For each record in table T2\n",
    "    for s in T2:\n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = H(s)\n",
    "        if s_key in dic:\n",
    "            dic[s_key].add(s) # If there is an entry\n",
    "        else:\n",
    "            dic[s_key] = {s}\n",
    "    print(dic)\n",
    "    # For each record in table T1 (probing)\n",
    "    for r in T1:\n",
    "        # Hash the record based on join attribute value using H\n",
    "        r_key = H(r)\n",
    "        # If an index entry is found Then\n",
    "        if r_key in dic:\n",
    "            # Compare each record on this index entry with the record of table T1\n",
    "            for item in dic[r_key]:\n",
    "                if item[1] == r[1]:\n",
    "                    # Put the rsult\n",
    "                    result.append( (r[0], r[1], item[0]) )\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXx6eixAvwPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: {('Arts', 8)}, 6: {('Business', 15)}, 2: {('Health', 11), ('CompSc', 2)}, 3: {('Finance', 21), ('Dance', 12)}, 7: {('Engineering', 7)}, 1: {('Geology', 10)}, 9: {('IT', 18)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Adele', 8, 'Arts'), ('Ed', 11, 'Health'), ('Joanna', 2, 'CompSc')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the partitioned result\n",
    "HB_join(R,S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Parallel Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cpQYKvvH241"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def consolidate_result(results, result):\n",
    "    # This is called whenever HB_Join(T1, T2) returns a result.\n",
    "    # The results list is modified only by the main process, not the pool workers.\n",
    "    results.append(result)\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Partition T1 & T2 into sub-tables using range_partition().\n",
    "    # The number of the sub-tables must be the equal to the n_processor\n",
    "    T1_subsets = range_partition(T1, [10, 20])\n",
    "    T2_subsets = range_partition(T2, [10, 20])\n",
    "    \n",
    "    # Apply local join for each processor\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    for i in range(len(T1_subsets)):\n",
    "        result = pool.apply_async(HB_join, [T1_subsets[i], T2_subsets[i]])\n",
    "        output = result.get()\n",
    "        results.append(output)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJgTe8pVH_0z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: {('CompSc', 2)}, 7: {('Engineering', 7)}, 8: {('Arts', 8)}}\n",
      "{1: {('Geology', 10)}, 2: {('Health', 11)}, 3: {('Dance', 12)}, 6: {('Business', 15)}, 9: {('IT', 18)}}\n",
      "{3: {('Finance', 21)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Irene', 14),\n",
       " [('Joanna', 2, 'CompSc'), ('Adele', 8, 'Arts')],\n",
       " [('Ed', 11, 'Health')],\n",
       " []]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processor = 3\n",
    "DPBP_join(R, S, n_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Duplication Outer Join Algorithm (DOJA)\n",
    "\n",
    "In this task, you will implement a **Duplication Outer Join Algorithm (DOJA)**. This algorithm consist of four main steps.\n",
    "1. (Replication): Duplicate the small table \n",
    "2. (Local Inner Join): Perform inner join in each processor\n",
    "3. (Distribution): Reshuffle the inner join result based on R.x\n",
    "4. (Local Outer Join): Perform outer join between the initial table R and the inner join result\n",
    "\n",
    "Assume **n_processor=3** and the dataset is initially partition using the **hash** method. For joining technique, please use the provided **outer join** function for the inner join and outer join operation. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "R = [('Adele',8),('Bob',22),('Clement',16),('Dave',23),('Ed',11),\n",
    "     ('Fung',25),('Goel',3),('Harry',17),('Irene',14),('Joanna',2),\n",
    "     ('Kelly',6),('Lim',20),('Meng',1),('Noor',5),('Omar',19)]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "S = [('Arts',8),('Business',15),('CompSc',2),('Dance',12),('Engineering',7),\n",
    "     ('Finance',21),('Geology',10),('Health',11),('IT',18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works \n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute. \n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the value of the join attribute into the digits\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "    \n",
    "    # Calulate the sum of elemenets in the digits\n",
    "    return sum(digits)\n",
    "\n",
    "def outer_join(L, R, join=\"left\"):\n",
    "    \"\"\"outer join using Hash-based join algorithm\"\"\"\n",
    "    \n",
    "    if join == \"right\":\n",
    "        L, R = R, L\n",
    "    \n",
    "    # inner join\n",
    "    if join == \"inner\":\n",
    "        h_dic = {}\n",
    "        for r in R:\n",
    "            h_r = H(r)\n",
    "            if h_r in h_dic.keys():\n",
    "                h_dic[h_r].add(r)\n",
    "            else:\n",
    "                h_dic[h_r] = {r}\n",
    "\n",
    "        result = []\n",
    "        for l in L:\n",
    "            h_l = H(l)\n",
    "            if h_l in h_dic.keys():\n",
    "                for item in h_dic[h_l]:\n",
    "                    if item[1] == l[1]:\n",
    "                        result.append([l[0], item[1], item[0]])\n",
    "        return result\n",
    "    \n",
    "    elif join in [\"left\", \"right\"]:\n",
    "        h_dic = {}\n",
    "        for r in R:\n",
    "            print(r)\n",
    "            h_r = H(r)\n",
    "            if h_r in h_dic.keys():\n",
    "                h_dic[h_r].add(r)\n",
    "            else:\n",
    "                h_dic[h_r] = {r}\n",
    "        print(h_dic)\n",
    "                \n",
    "        result = []\n",
    "        for l in L:\n",
    "            isFound = False # to check whether there is a match found.\n",
    "            h_l = H(l)\n",
    "\n",
    "            if h_l in h_dic.keys():\n",
    "                for item in h_dic[h_l]:\n",
    "                    if item[1] == l[1]:\n",
    "                        result.append([l[0], item[1], item[2]])\n",
    "                        isFound = True\n",
    "                        break\n",
    "                        \n",
    "            if not isFound:\n",
    "                result.append([l[0], l[1],str(np.nan)])\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        raise AttributeError('join should be in {left, right, inner}.') \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_distribution(T, n):\n",
    "    \"\"\"distribute data using hash partitioning\"\"\"\n",
    "    \n",
    "    # Define a simple hash function for demonstration\n",
    "    def s_hash(x, n):\n",
    "        h = x%n \n",
    "        return h\n",
    "    \n",
    "    result = {}\n",
    "    for t in T:\n",
    "        h_key = s_hash(t[1], n)\n",
    "        if h_key in result.keys():\n",
    "            result[h_key].add(tuple(t))\n",
    "            \n",
    "        else:\n",
    "            result[h_key] = {tuple(t)}\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def DOJA(L, R, n):\n",
    "    \"\"\"left outer join using DOJA\"\"\"\n",
    "\n",
    "    ### Start CODE\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    # Compare to see which is the shorter and longer list between R and S\n",
    "    # Temporary place inputted list then may change later\n",
    "    longer_list = R # S list\n",
    "    shorter_list = L  # R list\n",
    "\n",
    "    if len(L) < len(R):\n",
    "        longer_list = R\n",
    "        shorter_list = L \n",
    "        print(\"List L is shorter than List R\")\n",
    "    elif len(L) > len(R):\n",
    "        longer_list = L\n",
    "        shorter_list = R \n",
    "        print(\"List R is shorter than List L\") \n",
    "    else:\n",
    "        print(\"Both lists are equal length\") # Can randomly assign either of list to be the 'shorter' side\n",
    "    \n",
    "    # Distribute longer list to n number of processors using hash distribution\n",
    "    left_distribution = hash_distribution(longer_list, n)\n",
    "    print(\"left distribution\", left_distribution)\n",
    "    \n",
    "    # Create a new distribution with the shorter list added to each processor like in a form of [duplicated short list][left distribution]\n",
    "    duplicated_list = []\n",
    "    for dup in range(n):\n",
    "        duplicated_list.append(shorter_list)\n",
    "    \n",
    "    # INNER JOIN\n",
    "    pool = mp.Pool(n)\n",
    "    \n",
    "    results = []\n",
    "    for i in left_distribution.keys():\n",
    "        result = pool.apply_async(outer_join, [left_distribution[i], duplicated_list[i], \"inner\"])\n",
    "        # use async cus you want them to perform outer join at the same time\n",
    "        results.append(result)\n",
    "        \n",
    "    # REDISTRIBUTE \n",
    "    inner_peace = []\n",
    "    for x in results:\n",
    "        inner_peace.extend(x.get())\n",
    "        \n",
    "    # Reshuffle the inner join result\n",
    "    resuffled = hash_distribution(inner_peace, n)\n",
    "    print(\"Resuffled: \", resuffled)\n",
    "    \n",
    "    # Get the key\n",
    "    the_key = 0\n",
    "    for key in resuffled:\n",
    "        the_key = key\n",
    "    \n",
    "    # local outer join and combine\n",
    "    outputed = []\n",
    "    for i in left_distribution.keys():\n",
    "        outputs = pool.apply_async(outer_join, [left_distribution[i], resuffled[the_key], \"left\"])\n",
    "        print(\"left_distribution\", left_distribution)\n",
    "        print(\"resuffled\", resuffled)\n",
    "        # use async cus you want them to perform outer join at the same time in parallel\n",
    "        outputed.append(outputs)\n",
    "        \n",
    "    for y in outputed:\n",
    "        output.extend(y.get())\n",
    "\n",
    "    ### End CODE\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List R is shorter than List L\n",
      "left distribution {2: {('Dave', 23), ('Adele', 8), ('Ed', 11), ('Noor', 5), ('Harry', 17), ('Joanna', 2), ('Irene', 14), ('Lim', 20)}, 1: {('Omar', 19), ('Fung', 25), ('Meng', 1), ('Clement', 16), ('Bob', 22)}, 0: {('Kelly', 6), ('Goel', 3)}}\n",
      "('Adele', 8, 'Arts')('Adele', 8, 'Arts')\n",
      "\n",
      "('Joanna', 2, 'CompSc')('Joanna', 2, 'CompSc')\n",
      "\n",
      "('Ed', 11, 'Health')('Ed', 11, 'Health')\n",
      "\n",
      "{8: {('Adele', 8, 'Arts')}, 2: {('Joanna', 2, 'CompSc'), ('Ed', 11, 'Health')}}{8: {('Adele', 8, 'Arts')}, 2: {('Joanna', 2, 'CompSc'), ('Ed', 11, 'Health')}}\n",
      "\n",
      "('Adele', 8, 'Arts')\n",
      "('Joanna', 2, 'CompSc')\n",
      "('Ed', 11, 'Health')\n",
      "{8: {('Adele', 8, 'Arts')}, 2: {('Joanna', 2, 'CompSc'), ('Ed', 11, 'Health')}}\n",
      "Resuffled:  {2: {('Adele', 8, 'Arts'), ('Joanna', 2, 'CompSc'), ('Ed', 11, 'Health')}}\n",
      "left_distribution {2: {('Dave', 23), ('Adele', 8), ('Ed', 11), ('Noor', 5), ('Harry', 17), ('Joanna', 2), ('Irene', 14), ('Lim', 20)}, 1: {('Omar', 19), ('Fung', 25), ('Meng', 1), ('Clement', 16), ('Bob', 22)}, 0: {('Kelly', 6), ('Goel', 3)}}\n",
      "resuffled {2: {('Adele', 8, 'Arts'), ('Joanna', 2, 'CompSc'), ('Ed', 11, 'Health')}}\n",
      "left_distribution {2: {('Dave', 23), ('Adele', 8), ('Ed', 11), ('Noor', 5), ('Harry', 17), ('Joanna', 2), ('Irene', 14), ('Lim', 20)}, 1: {('Omar', 19), ('Fung', 25), ('Meng', 1), ('Clement', 16), ('Bob', 22)}, 0: {('Kelly', 6), ('Goel', 3)}}\n",
      "resuffled {2: {('Adele', 8, 'Arts'), ('Joanna', 2, 'CompSc'), ('Ed', 11, 'Health')}}\n",
      "left_distribution {2: {('Dave', 23), ('Adele', 8), ('Ed', 11), ('Noor', 5), ('Harry', 17), ('Joanna', 2), ('Irene', 14), ('Lim', 20)}, 1: {('Omar', 19), ('Fung', 25), ('Meng', 1), ('Clement', 16), ('Bob', 22)}, 0: {('Kelly', 6), ('Goel', 3)}}\n",
      "resuffled {2: {('Adele', 8, 'Arts'), ('Joanna', 2, 'CompSc'), ('Ed', 11, 'Health')}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Dave', 23, 'nan'],\n",
       " ['Adele', 8, 'Arts'],\n",
       " ['Ed', 11, 'Health'],\n",
       " ['Noor', 5, 'nan'],\n",
       " ['Harry', 17, 'nan'],\n",
       " ['Joanna', 2, 'CompSc'],\n",
       " ['Irene', 14, 'nan'],\n",
       " ['Lim', 20, 'nan'],\n",
       " ['Omar', 19, 'nan'],\n",
       " ['Fung', 25, 'nan'],\n",
       " ['Meng', 1, 'nan'],\n",
       " ['Clement', 16, 'nan'],\n",
       " ['Bob', 22, 'nan'],\n",
       " ['Kelly', 6, 'nan'],\n",
       " ['Goel', 3, 'nan']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOJA(R,S,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Parallel Join Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly answer the following question.\n",
    "\n",
    "1. For each partitioning algorithm besides range-partitioning, state and justify whether we can use it with the code for disjoint-partitioning based parallel join above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: Yes. Other partitioning algorithm like hash-partitioning and round-robin partitioning can be used with disjoint-partitioning based parallel joins. There is another partitioning algorithm which is the random-unequal data partitionining method but this method should not be used with disjoint-partitioning based parallel joins.\n",
    "\n",
    "For hash partitioning method, it distributes the tuples by implementing a hash function on the join key which is appropriate for parallel joins as it evenly spreads data among partitions, reducing the possibility of skewing. Disjoint-partitioning method can be used as long as the hash functions used in both hash partitioning and joining phases are compatible.\n",
    "\n",
    "For round-robin partitiioning method, it evenly distributes tuples among partitions by distributing them in a sequential manner whereby there is an equal spread of tuples among partitions and which is good for balancing workloads in parallel joins. It can work with disjoint-partitioning method because it is not dependent on the data value, which reduced the possibility of skewing.\n",
    "\n",
    "For random-unequal data partitionining method, it does not evenly distributes tuples among partitions into disjoint steps which is the main requirement for disjoint-partitioning based parallel joins. In addition to that, if random-unequal data partitionining method is used, there would be a high possibility of skewing as the data are randomly partitioned and the partitions are most likely unequal partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9vIqdbjJaXv"
   },
   "source": [
    "## 3. Parallel Sorting Algorithm (2 marks)\n",
    "\n",
    "In this task, you will implement **parallel binary-merge sort** method. It has two phases same as the parallel merge-all sort that you learnt in the labs.\n",
    "1. Local sort\n",
    "2. Final merge.\n",
    "\n",
    "The first phase is similar to the parallel merge-all sort. The second phase, the merging phase, is pipelined instead of concentrating on one processor. In this phase, we take the results from two processors and then merge the two in one processor, called binary merging. The result of the merge between two processors is passed on to the next level until one processor (the host) is left.\n",
    "\n",
    " **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n",
    "Assume that we use the round robin partitioning method  (i.e. `rr_partition()`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Round-robin Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5p8cRn73Zqc"
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # For each bin, \n",
    "    for name, number in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (name % n)\n",
    "        result[index_bin].append(number)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jk-Ch1Jm3iCC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Adele', 8), ('Dave', 23), ('Goel', 3), ('Joanna', 2), ('Meng', 1)],\n",
      " [('Bob', 22), ('Ed', 11), ('Harry', 17), ('Kelly', 6), ('Noor', 5)],\n",
      " [('Clement', 16), ('Fung', 25), ('Irene', 14), ('Lim', 20), ('Omar', 19)]]\n"
     ]
    }
   ],
   "source": [
    "# Test the round-robin partitioning function\n",
    "result = rr_partition(R, 3)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Serial Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd16AZF_LgWp"
   },
   "outputs": [],
   "source": [
    "def qsort(arr): \n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        # take the first element as the pivot\n",
    "        pivot = arr[0]\n",
    "        left_arr = [x for x in arr[1:] if x[1] < pivot[1]]\n",
    "        right_arr = [x for x in arr[1:] if x[1] >= pivot[1]]\n",
    "        # uncomment this to see what to print \n",
    "        # print(\"Left:\" + str(left_arr)+\" Pivot : \"+ str(pivot)+\" Right: \" + str(right_arr))\n",
    "        sorted_arr = qsort(left_arr) + [pivot] + qsort(right_arr)\n",
    "        \n",
    "        return sorted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8kIDuxV1uGS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Meng', 1),\n",
       " ('Joanna', 2),\n",
       " ('Goel', 3),\n",
       " ('Noor', 5),\n",
       " ('Kelly', 6),\n",
       " ('Adele', 8),\n",
       " ('Ed', 11),\n",
       " ('Irene', 14),\n",
       " ('Clement', 16),\n",
       " ('Harry', 17),\n",
       " ('Omar', 19),\n",
       " ('Lim', 20),\n",
       " ('Bob', 22),\n",
       " ('Dave', 23),\n",
       " ('Fung', 25)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qsort(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3vxcrs-LVaG"
   },
   "outputs": [],
   "source": [
    "# Let's first look at 'k-way merging algorithm' that will be used \n",
    "# to merge sub-record sets in our external sorting algorithm.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if(records[i][1] < m[1]):  \n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of multiple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for _ in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []\n",
    "    while(True):\n",
    "        merged_result = [] # the merging unit (i.e. # of the given buffers)\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                merged_result.append((None, sys.maxsize))\n",
    "            else:\n",
    "                merged_result.append(record_sets[i][indexes[i]])  \n",
    "        \n",
    "        # find the smallest record \n",
    "        smallest = find_min(merged_result)\n",
    "        \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(merged_result[smallest][1] == sys.maxsize):\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMrHqU8o3Mh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Adele', 8), ('Dave', 23), ('Goel', 3), ('Joanna', 2), ('Meng', 1)], [('Bob', 22), ('Ed', 11), ('Harry', 17), ('Kelly', 6), ('Noor', 5)], [('Clement', 16), ('Fung', 25), ('Irene', 14), ('Lim', 20), ('Omar', 19)]]\n",
      "[('Meng', 1),\n",
      " ('Joanna', 2),\n",
      " ('Goel', 3),\n",
      " ('Noor', 5),\n",
      " ('Kelly', 6),\n",
      " ('Adele', 8),\n",
      " ('Ed', 11),\n",
      " ('Irene', 14),\n",
      " ('Clement', 16),\n",
      " ('Harry', 17),\n",
      " ('Omar', 19),\n",
      " ('Lim', 20),\n",
      " ('Bob', 22),\n",
      " ('Dave', 23),\n",
      " ('Fung', 25)]\n"
     ]
    }
   ],
   "source": [
    "# Test k-way merging method\n",
    "buffers = rr_partition(R, 3)\n",
    "print(buffers)\n",
    "result = k_way_merge([qsort(b) for b in buffers])\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yof8Q84YLcOU"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of each sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Length of dataset\n",
    "    n = len(dataset)\n",
    "    \n",
    "    # --- Sort Phase ---\n",
    "    # Sort and divide the dataset into subsets\n",
    "    start_position = 0\n",
    "    sorted_set = []\n",
    "    while True:\n",
    "        if (n - start_position) > buffer_size:\n",
    "            subset = dataset[start_position: start_position + buffer_size]\n",
    "            sorted_subset = qsort(subset)\n",
    "            sorted_set.append(sorted_subset)\n",
    "            start_position += buffer_size\n",
    "        else:\n",
    "            subset = dataset[start_position:]\n",
    "            sorted_subset = qsort(subset)\n",
    "            sorted_set.append(sorted_subset)\n",
    "            break\n",
    "\n",
    "    # --- Merge Phase ---\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    while True:\n",
    "        merged_set = []\n",
    "        n = len(sorted_set)\n",
    "        start_position = 0\n",
    "        while True:\n",
    "            if (n - start_position) > merge_buffer_size:\n",
    "                subset = sorted_set[start_position: start_position + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset))\n",
    "                start_position += merge_buffer_size\n",
    "            else:\n",
    "                subset = sorted_set[start_position:]\n",
    "                merged_set.append(k_way_merge(subset))\n",
    "                break\n",
    "\n",
    "        sorted_set = merged_set\n",
    "        if len(sorted_set) <= 1:\n",
    "            result = merged_set\n",
    "            break\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adele', 8), ('Bob', 22), ('Clement', 16), ('Dave', 23), ('Ed', 11), ('Fung', 25), ('Goel', 3), ('Harry', 17), ('Irene', 14), ('Joanna', 2), ('Kelly', 6), ('Lim', 20), ('Meng', 1), ('Noor', 5), ('Omar', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmW47ALF6bH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sorting result:[[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "result = serial_sorting(R, 4)\n",
    "print(\"final sorting result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Parallel Binary-merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_jH8jXwLKRT"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel binary-merge sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Call rr_partition to do round-robin partitioning\n",
    "    round_robin = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # pool for parallel processing\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    \n",
    "    # Sort phase\n",
    "    sorted_dataset = []\n",
    "    for elem in round_robin:\n",
    "        # call serial sorting method\n",
    "        print(\"elem: \", elem)\n",
    "        sorted_dataset.append(*pool.apply_async(serial_sorting, [elem, buffer_size]).get())\n",
    "        print(\"sorted_dataset: \", sorted_dataset)\n",
    "    pool.close()\n",
    "    \n",
    "    # Merge phase\n",
    "    # For loop for binary merge\n",
    "    while len(sorted_dataset) > 1:\n",
    "        updated = []\n",
    "        for i in range(0, len(sorted_dataset), 2):\n",
    "            if i + 1 < len(sorted_dataset):\n",
    "                merged = k_way_merge([sorted_dataset[i], sorted_dataset[i + 1]])\n",
    "                updated.append(merged)\n",
    "            else:\n",
    "                # If there's an odd number of sorted datasets, just append the last one\n",
    "                updated.append(sorted_dataset[i])\n",
    "        sorted_dataset = updated\n",
    "    \n",
    "    # The final merged result\n",
    "    result = sorted_dataset[0]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMu19MwXLxNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elem:  [('Adele', 8), ('Kelly', 6)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)]]\n",
      "elem:  [('Bob', 22), ('Lim', 20)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)]]\n",
      "elem:  [('Clement', 16), ('Meng', 1)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)]]\n",
      "elem:  [('Dave', 23), ('Noor', 5)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)]]\n",
      "elem:  [('Ed', 11), ('Omar', 19)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)]]\n",
      "elem:  [('Fung', 25)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)], [('Fung', 25)]]\n",
      "elem:  [('Goel', 3)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)], [('Fung', 25)], [('Goel', 3)]]\n",
      "elem:  [('Harry', 17)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)], [('Fung', 25)], [('Goel', 3)], [('Harry', 17)]]\n",
      "elem:  [('Irene', 14)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)], [('Fung', 25)], [('Goel', 3)], [('Harry', 17)], [('Irene', 14)]]\n",
      "elem:  [('Joanna', 2)]\n",
      "sorted_dataset:  [[('Kelly', 6), ('Adele', 8)], [('Lim', 20), ('Bob', 22)], [('Meng', 1), ('Clement', 16)], [('Noor', 5), ('Dave', 23)], [('Ed', 11), ('Omar', 19)], [('Fung', 25)], [('Goel', 3)], [('Harry', 17)], [('Irene', 14)], [('Joanna', 2)]]\n",
      "final result:[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_binary_merge_sorting(R, 10, 20)\n",
    "print(\"final result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Parallel Binary-merge Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly answer the following.\n",
    "\n",
    "1. Does parallel binary-merge utilize all available processors? State and justify with respect to each phase: sort, merge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: Yes, parallel binary-merge do utilize all available processors whereby all the available processors can be used to sort different data subsets in parallel during the sorting phase and during the merge phase, the binary-merge algorithm merges two sorted lists at a time but when merging, the number of lists to merge reduces so less and less processors will be used so in conclusion, although all processors can be utilized at the sorting and merging phase, the merging phase may not fully use all the processors as it goes on as the workload has decreased as it goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Parallel GroupBy (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Parallel GroupBy with Merge-All\n",
    "\n",
    "In this task, you will implement **Parallel GroupBy with Merge-All** method. It invloves the following two steps:\n",
    "1. Local aggregate in each processor\n",
    "2. Global aggregation\n",
    "\n",
    "Assume **n_processor=4** and the dataset has already been pre-partitioned. For local aggregation, please use the provided **local_groupby** function. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = [('A', 1), ('B', 2), ('C', 3), ('A', 10), ('B', 20), ('C', 30)]\n",
    "D2 = [('A', 2), ('B', 3), ('C', 4), ('A', 20), ('B', 30), ('C', 40)]\n",
    "D3 = [('A', 3), ('B', 4), ('C', 5), ('A', 30), ('B', 40), ('C', 50)]\n",
    "D4 = [('A', 4), ('B', 5), ('C', 6), ('A', 40), ('B', 50), ('C', 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = 0\n",
    "        dict[key] += val\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 11, 'B': 22, 'C': 33}\n"
     ]
    }
   ],
   "source": [
    "result = local_groupby (D1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    " \n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in dataset:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby, [s]))\n",
    "    pool.close()\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    # Assume that the global operator is sum\n",
    "    for r in local_result:\n",
    "        for key, val in r.items():\n",
    "            if key not in result:\n",
    "                result[key] = 0\n",
    "            result[key] += val\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 110, 'B': 154, 'C': 198}\n"
     ]
    }
   ],
   "source": [
    "E = [D1, D2, D3, D4]\n",
    "result = parallel_merge_all_groupby (E)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Redistribution Method\n",
    "\n",
    "In this task, you will implement **Parallel GroupBy with Redistribution Method** method. It invloves the following two steps:\n",
    "1. (Partitioning phase): Redistribute raw records to all processor\n",
    "2. (Aggregation phase): Each processor performs a local aggregation\n",
    "\n",
    "\n",
    "Assume **n_processor=4** and the dataset should be partitioned into 4 groups using **range partition** Where first group with A & B, second with C & D, third with E & F and fourth with G & H. For local aggregation, please use the provided **local_groupby** function. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [('A', 1), ('B', 2), ('C', 3), ('D', 10), ('E', 20), ('F', 30), ('G', 20), ('H', 30), \n",
    "      ('A', 2), ('B', 3), ('C', 4), ('D', 20), ('E', 30), ('F', 40), ('G', 30), ('H', 40),\n",
    "      ('A', 3), ('B', 4), ('C', 5), ('D', 30), ('E', 40), ('F', 50), ('G', 40), ('H', 50),\n",
    "      ('A', 4), ('B', 5), ('C', 6), ('D', 40), ('E', 50), ('F', 60), ('G', 50), ('H', 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create range partition function based on GroupBy attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list of tuple)\n",
    "    range_indices -- the range end-points (e.g., ['C', 'E', 'G'] means\n",
    "                        4 intervals: x < 'C', 'C' <= x < 'E', 'E' <= x < 'G', 'G' <= x)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ###  \n",
    "    \n",
    "    # First, we sort the dataset according to their values\n",
    "    new_data = sorted(data, key=lambda x: x[1])\n",
    "\n",
    "    # Calculate the number of bins\n",
    "    n_bin = len(range_indices) + 1\n",
    "\n",
    "    # For each bin, perform the following\n",
    "    for i in range(n_bin):\n",
    "        # Find elements belonging to each range\n",
    "        if i == 0:\n",
    "            s = [x for x in new_data if x[0] in ('A', 'B')]\n",
    "        elif i == n_bin - 1:\n",
    "            s = [x for x in new_data if x[0] in ('G', 'H')]\n",
    "        else:\n",
    "            s = [x for x in new_data if range_indices[i-1] <= x[0] < range_indices[i]]\n",
    "        \n",
    "        # Add the partitioned list to the result\n",
    "        result.append(s)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_redistributed_groupby(dataset, range_indices):\n",
    "    \"\"\"\n",
    "    Perform a parallel redistributed groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # call range partition \n",
    "    ranged = range_partition(dataset, range_indices)\n",
    "\n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    " \n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    result = []\n",
    "    for each in ranged:\n",
    "        # call the local aggregation method\n",
    "        result.append(pool.apply(local_groupby, [each]))\n",
    "    pool.close()\n",
    "\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'A': 10, 'B': 14}, {'C': 18, 'D': 100}, {'E': 140, 'F': 180}, {'G': 140, 'H': 180}]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_redistributed_groupby (D, ['C', 'E', 'G'])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT3182 - Take_Home_Test_Solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
