{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b834463",
   "metadata": {},
   "source": [
    "<div style= \"text-align: right\">\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: x-large;\">FIT3182 Big Data Management and Processing</p>\n",
    "    <p style= \"text-align: right; font-weight: bold; font-size: large;\">Assignment 2</p>\n",
    "    <p style= \"text-align: right\">Foo Kai Yan</p>\n",
    "    <p style= \"text-align: right\">kfoo0012@student.monash.edu<br><br><i>33085625<br><br><i>5<sup>th</sup> May 2024</i></p>\n",
    "<div>\n",
    "<hr style=\"border-color: black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a88376",
   "metadata": {},
   "source": [
    "## Student Statement\n",
    "The assignment was completed with the assistance of some code obtained from seminar/tutorial/lab/applied class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce768c0",
   "metadata": {},
   "source": [
    "### Installing PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5bc036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.8/site-packages (4.3.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from pymongo) (2.3.0)\n",
      "Requirement already satisfied: pygeohash in /opt/conda/lib/python3.8/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install pygeohash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c6d7f",
   "metadata": {},
   "source": [
    "### Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f69ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pygeohash as pgh\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf # spark\n",
    "from pyspark.streaming import StreamingContext # spark streaming\n",
    "from pyspark.sql.functions import col, split, element_at, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791d2db",
   "metadata": {},
   "source": [
    "### Check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07df9969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/student/kfoo0012'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb931ae4",
   "metadata": {},
   "source": [
    "### Set os environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f01459",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f953f",
   "metadata": {},
   "source": [
    "### Set host ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89789e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hostip obtained using `ipconfig` command in command prompt\n",
    "hostip = \"192.168.68.58\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61252c",
   "metadata": {},
   "source": [
    "### Streaming Application Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada853fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Climate, Hotspot_AQUA, Hotspot_TERRA\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]') # local[*] MEANS you are using all the available processors\n",
    "    .appName('Streaming Climate Data')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc59567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is basically connecting to kafka server\n",
    "topic_stream_df = (\n",
    "    spark.readStream.format('kafka') # 'kafka' because u are receiving from kafka\n",
    "    .option('kafka.bootstrap.servers', f'{hostip}:9092')\n",
    "    .option('subscribe', topic)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78c6972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sdf = topic_stream_df.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35284526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtoDatabase_OLD(batch_df, batch_id):\n",
    "    all_data = batch_df.collect()  # Returns all elements as an array\n",
    "    # Send stream data to be transformed & analysed\n",
    "    producer_data = [row.asDict() for row in all_data]\n",
    "#     new_data = process_producer_data(data)\n",
    "    for eachdata in range(len(producer_data)): \n",
    "        temp_var = producer_data[eachdata].get('value')\n",
    "        # print(\"eachdata: \", eachdata)\n",
    "        # Convert byte array to string\n",
    "        newdata = temp_var.decode('utf-8')\n",
    "        # Convert string to JSON\n",
    "        produced_data = json.loads(newdata)\n",
    "        producer_data[eachdata] = produced_data\n",
    "        print(\"produced_data: \", producer_data)\n",
    "\n",
    "def addtoDatabase_old(batch_df, batch_id):\n",
    "    all_data = batch_df.collect()  # Returns all elements as an array\n",
    "    # Send stream data to be transformed & analysed\n",
    "    producer_data = [row.asDict() for row in all_data]\n",
    "\n",
    "    for eachdata in range(len(producer_data)):\n",
    "        # Check if 'value' key exists and is a bytearray\n",
    "        if 'value' in producer_data[eachdata] and isinstance(producer_data[eachdata]['value'], bytearray):\n",
    "            # Convert byte array to string\n",
    "            newdata = producer_data[eachdata]['value'].decode('utf-8')\n",
    "            # Convert string to JSON\n",
    "            produced_data = json.loads(newdata)\n",
    "            producer_data[eachdata] = produced_data\n",
    "        # If 'value' is not a bytearray, keep the original dictionary\n",
    "\n",
    "    print(\"produced_data: \", producer_data)\n",
    "    \n",
    "    for producer in producer_data:\n",
    "        producer_id = producer[\"producer_id\"]\n",
    "\n",
    "        climate_record = {}\n",
    "        aqua_hotspots_record = []\n",
    "        terra_hotspots_record = []\n",
    "\n",
    "        # Sort data depending on the producer_id information\n",
    "        if producer_id == \"producer1_climate\":\n",
    "            climate_record = producer\n",
    "            print(\"climate record\", climate_record)\n",
    "        elif producer_id == \"producer2_hotspot_aqua\":\n",
    "            aqua_hotspots_record.append(producer)\n",
    "            print(\"aqua_hotspots_record\", aqua_hotspots_record)\n",
    "        elif producer_id == \"producer3_hotspot_terra\":\n",
    "            terra_hotspots_record.append(producer)\n",
    "            print(\"terra_hotspots_record\", terra_hotspots_record)\n",
    "            \n",
    "        hotspots = []\n",
    "        \n",
    "        \n",
    "            \n",
    "        # Get Climate longitude and latitude\n",
    "        climate_latitude = climate_record.get(\"latitude\")\n",
    "        climate_longitude = climate_record.get(\"longitude\") \n",
    "\n",
    "#         print(\"climate_latitude: \", climate_latitude)\n",
    "#         print(\"climate_longitude: \", climate_longitude)\n",
    "        \n",
    "        # Process aqua_hotspots_record\n",
    "        for each_record in aqua_hotspots_record:\n",
    "            #if geohash_proximity(each_record, climate_record):\n",
    "            if (pgh.encode(each_record[\"longitude\"], each_record[\"latitude\"], precision = 3) == pgh.encode(climate_record.get(\"longitude\"), climate_record.get(\"latitude\"), precision = 3)):\n",
    "                #each_record[\"date\"] = climate_record.get(\"date\")\n",
    "                hotspots.append(each_record)\n",
    "                print(\"hotspot\", hotspots)\n",
    "\n",
    "def addtoDatabase(batch_df, batch_id): # NO ERROR OUH YEAAA\n",
    "    all_data = batch_df.collect()  # Returns all elements as an array\n",
    "    producer_data = [row.asDict() for row in all_data]\n",
    "\n",
    "    climate_record = [] #{}\n",
    "    aqua_hotspots_record = []\n",
    "    terra_hotspots_record = []\n",
    "    hotspots = []\n",
    "\n",
    "    for eachdata in range(len(producer_data)):\n",
    "        # Check if 'value' key exists and is a bytearray\n",
    "        if 'value' in producer_data[eachdata] and isinstance(producer_data[eachdata]['value'], bytearray):\n",
    "            # Convert byte array to string and then to JSON\n",
    "            newdata = producer_data[eachdata]['value'].decode('utf-8')\n",
    "            produced_data = json.loads(newdata)\n",
    "            producer_data[eachdata] = produced_data\n",
    "\n",
    "    for producer in producer_data:\n",
    "        producer_id = producer[\"producer_id\"]\n",
    "\n",
    "#         climate_record = {}\n",
    "#         aqua_hotspots_record = []\n",
    "#         terra_hotspots_record = []\n",
    "\n",
    "        # Sort data depending on the producer_id information\n",
    "        if producer_id == \"producer1_climate\":\n",
    "            climate_record.append(producer)\n",
    "            # print(\"climate record\", climate_record)\n",
    "        elif producer_id == \"producer2_hotspot_aqua\":\n",
    "            aqua_hotspots_record.append(producer)\n",
    "            # print(\"aqua_hotspots_record\", aqua_hotspots_record)\n",
    "        elif producer_id == \"producer3_hotspot_terra\":\n",
    "            terra_hotspots_record.append(producer)\n",
    "            # print(\"terra_hotspots_record\", terra_hotspots_record)\n",
    "\n",
    "    # If there are no climate records, skip processing this batch\n",
    "    # Check if the dictionary is empty\n",
    "    if len(climate_record) == 1:\n",
    "        climate_record = climate_record\n",
    "    elif len(climate_record) > 1:\n",
    "        climate_record = climate_record[0]\n",
    "    else:\n",
    "        # print(\"No climate record present in this batch. Skipping...\")\n",
    "        return  # Exit the function\n",
    "\n",
    "    # If there is a climate record, continue processing\n",
    "    # ... rest of your code to process the records ...\n",
    "\n",
    "#     print(\"Climate records:\", climate_record)\n",
    "#     print(\"Aqua hotspots records:\", aqua_hotspots_record)\n",
    "#     print(\"Terra hotspots records:\", terra_hotspots_record)\n",
    "    # Further processing can be done here, such as combining data or storing in a database\n",
    "\n",
    "    # Process terra_hotspots_record\n",
    "    print(\"out loop\")\n",
    "    for each_record in terra_hotspots_record:\n",
    "        print(\"in loop terra\")\n",
    "        if geohash_proximity(each_record, climate_record):\n",
    "            #each_record[\"date\"] = climate_record.get(\"date\")\n",
    "            hotspots.append(each_record)\n",
    "            print(\"hotspot\", hotspots)\n",
    "            \n",
    "    for each_record in aqua_hotspots_record:\n",
    "        print(\"in loop aqua\")\n",
    "        if geohash_proximity(each_record, climate_record):\n",
    "            #each_record[\"date\"] = climate_record.get(\"date\")\n",
    "            hotspots.append(each_record)\n",
    "            print(\"hotspot\", hotspots)\n",
    "            \n",
    "    # Analyse hotspots data, find if any are close by & merge\n",
    "    new_hotspots_record = hotspots\n",
    "    # Merge hotspots with climate depending if close & label if natural or other\n",
    "    new_climate_record = process_climate(climate_record, new_hotspots_record)\n",
    "            \n",
    "    return hotspots\n",
    "\n",
    "# Check proximity based on geohash, precision 3\n",
    "def geohash_proximity(record, climate):\n",
    "    record_geohash = pgh.encode(record[\"longitude\"], record[\"latitude\"], precision = 3)\n",
    "    climate_geohash = pgh.encode(climate[0][\"longitude\"], climate[0][\"latitude\"], precision = 3)\n",
    "    return record_geohash == climate_geohash\n",
    "\n",
    "    # Only save data with contents\n",
    "#     if len(new_data) > 1:\n",
    "\n",
    "#         client = MongoClient(f'mongodb://{hostip}:27017/')\n",
    "\n",
    "#         db = client.fit3182_assignment_db\n",
    "#         climate_collection = db.climate_collection\n",
    "\n",
    "#         # Insert climate data into database\n",
    "#         climate_collection.insert_one(new_data)\n",
    "#         pprint(new_data)\n",
    "\n",
    "#         client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d4acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_writer = (\n",
    "    data_sdf\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    # collect data for 10 seconds\n",
    "    .trigger(processingTime='10 seconds')\n",
    "    # each batch of 10 seconds will run func\n",
    "    .foreachBatch(addtoDatabase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abc79ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out loop\n",
      "in loop terra\n",
      "in loop terra\n",
      "in loop terra\n",
      "hotspot [{'latitude': -37.9029, 'longitude': 143.5946, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'created_time': '10:14:53', 'producer_id': 'producer3_hotspot_terra'}]\n",
      "in loop terra\n",
      "hotspot [{'latitude': -37.9029, 'longitude': 143.5946, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'created_time': '10:14:53', 'producer_id': 'producer3_hotspot_terra'}, {'latitude': -37.636, 'longitude': 149.33, 'confidence': 94.0, 'surface_temperature_celcius': 43.0, 'created_time': '10:14:53', 'producer_id': 'producer3_hotspot_terra'}]\n",
      "in loop aqua\n",
      "hotspot [{'latitude': -37.9029, 'longitude': 143.5946, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'created_time': '10:14:53', 'producer_id': 'producer3_hotspot_terra'}, {'latitude': -37.636, 'longitude': 149.33, 'confidence': 94.0, 'surface_temperature_celcius': 43.0, 'created_time': '10:14:53', 'producer_id': 'producer3_hotspot_terra'}, {'latitude': -37.9009, 'longitude': 143.6217, 'confidence': 62.0, 'surface_temperature_celcius': 45.0, 'created_time': '12:38:51', 'producer_id': 'producer2_hotspot_aqua'}]\n",
      "in loop aqua\n",
      "hotspot [{'latitude': -37.9029, 'longitude': 143.5946, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'created_time': '10:14:53', 'producer_id': 'producer3_hotspot_terra'}, {'latitude': -37.636, 'longitude': 149.33, 'confidence': 94.0, 'surface_temperature_celcius': 43.0, 'created_time': '10:14:53', 'producer_id': 'producer3_hotspot_terra'}, {'latitude': -37.9009, 'longitude': 143.6217, 'confidence': 62.0, 'surface_temperature_celcius': 45.0, 'created_time': '12:38:51', 'producer_id': 'producer2_hotspot_aqua'}, {'latitude': -37.0636, 'longitude': 145.3614, 'confidence': 82.0, 'surface_temperature_celcius': 55.0, 'created_time': '12:38:51', 'producer_id': 'producer2_hotspot_aqua'}]\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = c929529b-12fb-45f9-8a4c-79eb58a9fb17, runId = 0aec3f9c-4149-4ba6-b953-a9bf26e35987] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 272, in call\n    raise e\n  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 269, in call\n    self.func(DataFrame(jdf, self.session), batch_id)\n  File \"/tmp/ipykernel_1307/3193697816.py\", line 142, in addtoDatabase\n    new_climate_record = process_climate(climate_record, new_hotspots_record)\nNameError: name 'process_climate' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     query \u001b[38;5;241m=\u001b[39m db_writer\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupted by CTRL-C. Stopping query.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Query [id = c929529b-12fb-45f9-8a4c-79eb58a9fb17, runId = 0aec3f9c-4149-4ba6-b953-a9bf26e35987] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 272, in call\n    raise e\n  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 269, in call\n    self.func(DataFrame(jdf, self.session), batch_id)\n  File \"/tmp/ipykernel_1307/3193697816.py\", line 142, in addtoDatabase\n    new_climate_record = process_climate(climate_record, new_hotspots_record)\nNameError: name 'process_climate' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = db_writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debcf08e",
   "metadata": {},
   "source": [
    "client = MongoClient(f'mongodb://{hostip}:27017/')\n",
    "db = client.fit3182_assignment_db\n",
    "cursor = db.climate_collection.find({})\n",
    "\n",
    "for document in cursor:\n",
    "    pprint(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38921b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check proximity based on geohash, precision 5\n",
    "def geohash_proximity_5(record, climate):\n",
    "    record_geohash = pgh.encode(record.get(\"longitude\"), record.get(\"latitude\"), precision = 5)\n",
    "    climate_geohash = pgh.encode(climate.get(\"longitude\"), climate.get(\"latitude\"), precision = 5)\n",
    "    return record_geohash == climate_geohash\n",
    "\n",
    "# Check proximity based on geohash, precision 3\n",
    "def geohash_proximity(record, climate):\n",
    "    record_geohash = pgh.encode(record.get(\"longitude\"), record.get(\"latitude\"), precision = 3)\n",
    "    climate_geohash = pgh.encode(climate.get(\"longitude\"), climate.get(\"latitude\"), precision = 3)\n",
    "    return record_geohash == climate_geohash\n",
    "\n",
    "def process_producer_data(producer_data):\n",
    "    print(\"producer_data: \", producer_data)\n",
    "    # Initialize empty lists for aqua_hotspots_record, terra_hotspots_record, and an empty dictionary for climate_record\n",
    "    climate_record = {}\n",
    "    aqua_hotspots_record = []\n",
    "    terra_hotspots_record = []\n",
    "    \n",
    "    # Producer 1: Climate topic_name = \"Climate\", [\"producer_id\"] = \"producer1_climate\"\n",
    "    # Producer 2: AQUA topic_name = \"Hotspot_AQUA\", [\"producer_id\"] = \"producer2_hotspot_aqua\"\n",
    "    # Producer 3: TERRA topic_name = \"Hotspot_TERRA\", [\"producer_id\"] = \"producer3_hotspot_terra\"\n",
    "    \n",
    "    # For each item from the data batch from the Kafka stream \n",
    "    for eachdata in range(len(producer_data)): \n",
    "        temp_var = producer_data[eachdata].get(\"value\")\n",
    "        # print(\"eachdata: \", eachdata)\n",
    "        # Convert byte array to string\n",
    "        newdata = temp_var.decode('utf-8')\n",
    "        # Convert string to JSON\n",
    "        produced_data = json.loads(newdata)\n",
    "        print(\"produced_data: \", produced_data)\n",
    "        producer_id = produced_data[\"producer_id\"]\n",
    "\n",
    "        # Sort data depending on the producer_id information\n",
    "        if producer_id == \"producer1_climate\":\n",
    "            climate_record = produced_data\n",
    "        elif producer_id == \"producer2_hotspot_aqua\":\n",
    "            aqua_hotspots_record.append(produced_data)\n",
    "        elif producer_id == \"producer3_hotspot_terra\":\n",
    "            terra_hotspots_record.append(produced_data)\n",
    "\n",
    "    # Analyse hotspots data, find if any are close by & merge\n",
    "    new_hotspots_record = process_hotspots(aqua_hotspots_record, terra_hotspots_record, climate_record)\n",
    "    # Merge hotspots with climate depending if close & label if natural or other\n",
    "    new_climate_record = process_climate(climate_record, new_hotspots_record)\n",
    "\n",
    "    return new_climate_record\n",
    "\n",
    "def process_hotspots(aqua_hotspots_record, terra_hotspots_record, climate_record):\n",
    "    # Initialize empty array for hotspots records from both terra and aqua\n",
    "    hotspots = []\n",
    "    \n",
    "    print(\"climate_record: \", climate_record)\n",
    "    \n",
    "    # Get Climate longitude and latitude\n",
    "    climate_latitude = climate_record.get(\"latitude\")\n",
    "    climate_longitude = climate_record.get(\"longitude\") \n",
    "    \n",
    "    print(\"climate_latitude: \", climate_latitude)\n",
    "    print(\"climate_longitude: \", climate_longitude)\n",
    "    \n",
    "    # Process aqua_hotspots_record\n",
    "    for each_record in aqua_hotspots_record:\n",
    "        if geohash_proximity(each_record, climate_record):\n",
    "            each_record[\"date\"] = climate_record.get(\"date\")\n",
    "            hotspots.append(each_record)\n",
    "            \n",
    "    # Process terra_hotspots_record\n",
    "    for each_record in terra_hotspots_record:\n",
    "        if geohash_proximity(each_record, climate_record):\n",
    "            each_record[\"date\"] = climate_record.get(\"date\")\n",
    "            hotspots.append(each_record)\n",
    "            \n",
    "    return hotspots\n",
    "\n",
    "def process_climate(climate_record, hotspots_record):\n",
    "    # Initialize empty array for fire events records\n",
    "    fire_events = []\n",
    "    for each_record in hotspots_record:\n",
    "        fire_event = process_fire_event(climate_record, each_record)\n",
    "        if fire_event is not None:\n",
    "            fire_events.append(fire_event)\n",
    "    climate_record[\"fire_events\"] = fire_events\n",
    "    return climate_record\n",
    "\n",
    "def process_fire_event(climate_record, hotspots_record):\n",
    "    # Initialize an empty dictionary to store fire event data\n",
    "    fire_happening = {}\n",
    "\n",
    "    # Check if the hotspot is geographically close to the climate data with a precision of 5\n",
    "    if geohash_proximity_5(hotspots_record, climate_record) is True:\n",
    "        \n",
    "        print(\"hotspots_record: \", hotspots_record)\n",
    "        \n",
    "        # Calculate the average surface temperature from both hotspot and climate data\n",
    "        average_temp = (hotspots_record.get(\"surface_temperature_celcius\") + hotspots_record.get(\"surface_temperature_celcius\")) / 2\n",
    "        fire_happening[\"average_surface_temp\"] = average_temp\n",
    "\n",
    "        # Calculate the average confidence level from both hotspot and climate data\n",
    "        average_confidence = (hotspots_record.get(\"confidence\") + hotspots_record.get(\"confidence\")) / 2\n",
    "        fire_happening[\"confidence\"] = average_confidence\n",
    "\n",
    "        # Determine the cause of the fire based on climate conditions\n",
    "        air_temp = climate_record.get(\"air_temperature_celcius\")\n",
    "        solar_irradiance = climate_record.get(\"GHI_w/m2\")\n",
    "        if air_temp > 20 and solar_irradiance > 180:\n",
    "            fire_happening[\"cause\"] = \"natural\"\n",
    "        else:\n",
    "            fire_happening[\"cause\"] = \"others\"\n",
    "        \n",
    "        print(\"fire_happening: \", fire_happening)\n",
    "\n",
    "        # Return the dictionary containing fire event data\n",
    "        return fire_happening\n",
    "    # If the hotspot is not close to the climate data, return None\n",
    "    return None\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
