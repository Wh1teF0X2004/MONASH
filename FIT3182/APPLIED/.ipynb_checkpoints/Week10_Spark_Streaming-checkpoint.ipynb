{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming (Consumer 6)\n",
    "This is a basic streaming application that demonstrates [Spark/Kafka integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html) when using [Spark structured streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) for stream processing. This means, in practical terms, that Spark is able to produce streaming DataFrames from Kafka topics, and operate on those DataFrames using most of the DataFrame transformation APIs. This makes streaming data processing look like batch data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To run this demo code, you shall have your mongodb server, zookeeper and kafka containers running.**  \n",
    "First, import our database client, spark session entry point, and some basic spark sql functions for data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, element_at, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a global variable to store topic name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'Scenario06'\n",
    "hostip = \"10.192.45.88\" # change it to your IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our spark session with `#threads = #logicalCPU` and the given application name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]') # local[*] MEANS you are using all the available processors\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a streaming dataframe with options providing the bootstrap server(s) and topic name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is basically connecting to kafka server\n",
    "topic_stream_df = (\n",
    "    spark.readStream.format('kafka') # 'kafka' because u are receiving from kafka\n",
    "    .option('kafka.bootstrap.servers', f'{hostip}:9092')\n",
    "    .option('subscribe', topic_name)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the schema for this dataframe to see what columns we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate our output stream.\n",
    "1. We cast `col('value')` as a string, split `col('value')` on the separator `':'`, and name the result column as `'data'`.\n",
    "1. We select the value at index 2 (1-based indexing) in `col('data')`. This will be the number (if present), or an empty string otherwise.\n",
    "1. We perform a conditional transformation of `col('data')`.\n",
    "  1. **if `col('data')` is the empty string**, we replace it with `'*'`\n",
    "  1. **else** do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stream_df = (\n",
    "    topic_stream_df\n",
    "    .select(                                      # 1\n",
    "        split(\n",
    "            topic_stream_df.value.cast('string'),\n",
    "            ':'\n",
    "        )\n",
    "        .alias('data')\n",
    "    )\n",
    "    .withColumn('data', element_at('data', 2))    # 2\n",
    "    .withColumn(\n",
    "        'data',\n",
    "        (\n",
    "            when( col('data') == '', '*' )        # 3A\n",
    "            .otherwise( col('data') )             # 3B\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the schema of the result dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility class to process the rows of our streaming dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbWriter:\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{hostip}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.mongo_client['fit3182_db']\n",
    "        return True\n",
    "    \n",
    "    # called once per row of the result dataframe\n",
    "    # the current code DOES NOT handle duplicate processing\n",
    "    #   e.g., query fails and restarts just before current micro-batch was fully inserted\n",
    "    def process(self, row):\n",
    "        self.db[topic_name].insert_one(row.asDict())\n",
    "    \n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define our stream writer for the MongoDB database sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_writer = (\n",
    "    output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(DbWriter())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional stream for debugging, we define a stream writer for the console sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_logger = (\n",
    "    output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `writer` points to the stream writer we wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this variable to either db_writer or console_logger to get output to the desired sink\n",
    "writer = db_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start our streaming query. The workflow is straightforward.\n",
    "1. Call the `StreamWriter` object's `start` method to begin execution of the query.\n",
    "1. The `query` variable allows us to manage the running query. This is a simple demo, so we use `awaitTermination` to block our driver program until the streaming query is stopped due to failure or user-interrupts.\n",
    "\n",
    "You can press \"interrupt the kernel\" to stop the running query anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
