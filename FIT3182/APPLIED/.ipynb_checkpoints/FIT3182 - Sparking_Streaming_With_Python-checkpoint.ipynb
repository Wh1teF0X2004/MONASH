{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRJa6F6sOEdl"
   },
   "source": [
    "# FIT3182 - Big data management and processing\n",
    "\n",
    "# Activity: Spark Streaming with Python#\n",
    "\n",
    "**Apache Spark** is a fast and general engine for large-scale data processing. It has been reported that Spark is **100x faster** than Hadoop MapReduce in memory and **10x faster** on disk. Apache Spark is designed to write applications quickly in Java, Scala or Python.\n",
    "\n",
    "**Apache Spark Streaming** makes it easy to build scalable fault-tolerant **streaming** applications. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.\n",
    "\n",
    "In this activity, we will first learn how to **write Spark Streaming programs in Python** using **discretized stream** or **DStreams** which represents a continuous stream of data. Then, we will introduce you to **Spark structured streaming**.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFRlMtRDOEdm"
   },
   "source": [
    "## 1. Overview ##\n",
    "\n",
    "### What is Apache Spark?\n",
    "Apache Spark is a fast and general engine for big data processing and a distributed processing framework.\n",
    "\n",
    "It aims to provide a big data processing framework that can be used for streaming data manipulation (Spark streaming), machine learing and batch processing (Hadoop integration). Spark introduces an **abstract common data format** used to for efficient data sharing across parallel computation - **RDD (Resilient Distributed Datasets)**.\n",
    "\n",
    "### What is Apache Spark Streaming?\n",
    "Spark Streaming provides a high-level abstraction called **discretized stream** or **DStream (a sequence of RDD)**, which represents a continuous stream of data. **Streaming data** can be brought from many difference live streams or sources (e.g. Twitter, Kafka). Then, the processed data can be manipulated and stored into a big database and/or published into Web pages.\n",
    "\n",
    "Processing streaming data is a new way of looking at and manipulating real-time streaming data which contradits batching processing. By processing streaming data, as one of the obvious benefits, we can reduce latency between an event occurring and taking an action driven by it.\n",
    "\n",
    "Once real-time input data streams are received, Spark Streaming divides the data into \"batches\", and then the Spark Engine process them. In this activity, we will learn and practice how we can manipulate input data streams in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "#### What is batch?\n",
    "- How long the data will be collected (time in seconds) before processing\n",
    "\n",
    "#### What is RDD?\n",
    "- Collection of data distributed across a cluster of machines\n",
    "- Think of it like a new type of format\n",
    "    - i.e., xml, json...\n",
    "- Data is static, doesn't change overtime\n",
    "\n",
    "#### What is DStream?\n",
    "- Continuous stream of data arriving in real time\n",
    "- Processed in mirco-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wIni8ocOEdo"
   },
   "source": [
    "## 2. Create Streaming Context ##\n",
    "\n",
    "### Our Example\n",
    "To explain the use of the Spark APIs of Python, we will demonstrate a simple example:  ***\"counting the number of words in input data streams\"***.\n",
    "\n",
    "Imagine we are receiving the input text data streams through a TCP socket from a certain data server, and we wish to count the number of words in the data.\n",
    "\n",
    "### SparkContext and StreamingContext\n",
    "Apache Spark community released a powerful Python package, **`pyspark`**. Using **`pyspark`**, we can  initialise Spark, load streaming data, create RDD  from the data, sort, filter and sample the data. \n",
    "\n",
    "Especially, we will use and import **`StreamingContext`** from **`pyspark`**, which is the main entry point for Spark Streaming functionality. The **`StreamingContext`** object provides methods used to create DStreams from various input sources.  \n",
    "\n",
    "Spark applications run as independent sets of processes on a cluster, which is specified by the **`SparkContext`** object. **`SparkContext`** can connect to several types of cluster managers (local (standalone), Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (passed to `SparkContext`) to the executors. Finally, **`SparkContext`** sends tasks to the executors to run.\n",
    "\n",
    "### Python Code\n",
    "Thus, we need to import these two context:\n",
    "\n",
    "```\n",
    "from pyspark import SparkContext # spark\n",
    "from pyspark.streaming import StreamingContext # spark streaming\n",
    "```\n",
    "\n",
    "As mentioned, **`SparkContext`** is the main object under which everything else can be used. Then, we need to pass this object with a batch interval (in this example, we use **10 seconds**) into the **`StreamingContext`** object. By doing so, we're ready to create our own stream context via `StreamingContext`:\n",
    "\n",
    "```\n",
    "# Create a local StreamingContext with as many working processors as possible and a batch interval of 10 seconds            \n",
    "batch_interval = 10\n",
    "\n",
    "# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "sc = SparkContext(master=\"local[*]\", appName = \"WordCountApp\") \n",
    "\n",
    "# a batch interval of 10 seconds   \n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "```\n",
    "\n",
    "In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). If we want Spark to run locally with 'k' worker threads, we can specify as `local[k]`.\n",
    "\n",
    "The `appName` field is a name to be shown on the Sparking cluster UI. The batch interval (i.e. `batch_interval`) must be set based on the latency requirements of your application and available cluster resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxaEY6_ROEdp"
   },
   "source": [
    "## 3. Create DStream Data\n",
    "\n",
    "Once a `StreamingContext` (i.e. `ssc`) is defined, we can now define a DStreams representing the streaming data that can be received from a data server through a TCP socket. This server is specified in the method `ssc.socketTextStream(host, port)`, where `host` indicates the host name and `port` is its port number. With this example, the host is the local host and the port is 9999.\n",
    "\n",
    "```\n",
    "# Create a DStream connecting to hostname:port\n",
    "host = \"localhost\"\n",
    "port = 9999\n",
    "lines = ssc.socketTextStream(host, port)\n",
    "```\n",
    "\n",
    "The variable `lines` represents the stream of data (i.e. DStream) that will be received from the data server. A unit record in this data corresponds to a line of text. \n",
    "\n",
    "To count the number of the words in each line, we may want to define a function that can split the line into words. With this example, we use a lambda function;\n",
    "\n",
    "```\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "```\n",
    "\n",
    "`flatMap` is a one-to-many DStream operation. It creates a new DStream by generating multiple new records from each record. Thus, each line will be split into multiple words and we create a new DStream which is the stream of words. \n",
    "\n",
    "Now we further create a DStream of pairs (ie. the `pairs` DStream consisting of (word, count) pairs). For this purpose, we can use `reduceByKey` transformation for counting the number of each word in the `pairs` DStream. We can implement as follows:\n",
    "```\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "# Print the counting result\n",
    "wordCounts.pprint()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8j_BnxnOEdp"
   },
   "source": [
    "## 4. Run Sparking Stream\n",
    "\n",
    "Note that up to now, we have only established a computation environment for our Spark Streaming example. Thus, no real processing has started yet. To start processing, we need to perform the following code:\n",
    "\n",
    "```\n",
    "# Start the computation\n",
    "ssc.start()             \n",
    "# Wait for the computation to terminate. \n",
    "# We have added a `timeout` to deliberately cancel the execution after one minute. \n",
    "# In practice, you would not set this.\n",
    "\n",
    "try:\n",
    "    ssc.awaitTermination(timeout=60)  \n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    \n",
    "# If we want to manually stop the streaming context, use the following.\n",
    "ssc.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYYRgKLQpUDM"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eh9d9sODOEdq"
   },
   "source": [
    "### Important Note\n",
    "We need to wrap up all the above code snippets as below. It is a **Streaming Client** program. This program counts the words in the line sent by the **Streaming Server** application. Before running the **Streaming Client**, we need to run a **Streaming Server** application. Please download **FIT3182 - TCP_Server.ipnyb** file from Moodle and open it in another tab. Run the **FIT3182 - TCP_Server.ipnyb** code. Then, run the code below. The lines sent from the TCP Server will be counted and printed on this browser every 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLgI2bKaOEdr",
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3880.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:568)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:763)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:576)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#! reduceByKey --> groups the tuple with the same key and reduces them with the function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# we want to sum the values associated with each key\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Print the result                            \u001b[39;00m\n\u001b[1;32m     43\u001b[0m wordCounts\u001b[38;5;241m.\u001b[39mpprint()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     ssc\u001b[38;5;241m.\u001b[39mawaitTermination(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/streaming/context.py:214\u001b[0m, in \u001b[0;36mStreamingContext.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    Start the execution of the streams.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     StreamingContext\u001b[38;5;241m.\u001b[39m_activeContext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3880.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:568)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:763)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:576)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:05:40\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('0', 2)\n",
      "('1', 3)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:05:50\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('0', 1)\n",
      "('1', 1)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 3)\n",
      "('is', 10)\n",
      "('2', 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". \n",
    "# If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Create a local StreamingContext with as many working processors as possible \n",
    "# and a batch interval of 10 seconds            \n",
    "batch_interval = 10\n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "#! 'local[*]' = use all available processors\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[*]\", appName = \"WordCountApp\")\n",
    "ssc = StreamingContext(sc, batch_interval) # sc = spark context\n",
    " \n",
    "host = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "# IMPORTANT\n",
    "lines = ssc.socketTextStream(host, int(port))\n",
    "# This is to create the DStream (continuous stream of data)\n",
    "\n",
    "# THE ACTUAL WORK WE DOING: \"counting the number of words in input data streams\"\n",
    "# Useful for transforming streams of text data (sentence/paragraph/post/tweet) into streams\n",
    "# Individual words/token. Delimiter is space ' '\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \")) # Transform DStream input to DStream Object\n",
    "#! flatmap() applies a given function to EACH ELEMENT of the RDD/DStream object\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "#! map() applied a given function to EACH ELEMENT of the RDD/DStream object\n",
    "# BUT resulting output is often same length as input\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "#! reduceByKey --> groups the tuple with the same key and reduces them with the function\n",
    "# we want to sum the values associated with each key\n",
    "\n",
    "# Print the result                            \n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "try:\n",
    "    ssc.awaitTermination(timeout=20)\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "#except Exception as e:\n",
    "#    print(f'Error at starting context: {e}')\n",
    "#finally:\n",
    "#    ssc.stop(stopSparkContext = True, stopGracefully = True)\n",
    "\n",
    "#ssc.stop()\n",
    "#sc.stop()\n",
    "\n",
    "# Usually, u need to stop the ssc but you can set the timeout termination with .awaitTermination(timeout=60) where it will stop after 60seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MMg9fu_OEdw"
   },
   "source": [
    "## 5. Concepts in Sparking Streaming##\n",
    "\n",
    "Now we will learn some basic concepts in Spark Streaming. \n",
    "\n",
    "### Discretized Streams (DStreams)\n",
    "As mentioned above, **DStream** is the basic abstraction in Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. \n",
    "\n",
    "A DStream is seen as a continuous series of **RDDs**, which is Spark's abstraction of an immutable, distributed dataset (see [Spark Programming](https://spark.apache.org/docs/latest/rdd-programming-guide.html) to learn its more details). Each RDD in a DStream contains data from a certain interval.\n",
    "\n",
    "Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in our above example, the `flatMap` operation is applied on each RDD to generate the RDDs of the `words` DStream. \n",
    "\n",
    "\n",
    "### Transformations on DStreams\n",
    "We can apply various transformation operations on a DStream to modify its structure. Below, we see some of these transformations.\n",
    "\n",
    "#### UpdateStateByKey Operation\n",
    "This operation allows us to maintain **arbitrary state** while continuously updating it with new information. \n",
    "\n",
    "In order to use this operation, we need to do the following: \n",
    "    1. Define the state\n",
    "    2. Define the state update function: specify with a function how to update the state \n",
    "\n",
    "To illustrate, let's get back to our previous example. Now we want to keep a count of each word seen in a text data stream. Here, **the running count is the state** and we will use **the `updateStateByKey` operation** for this update purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIPIfk5TOEdx"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# add the new values with the previous running count to get the new count\n",
    "def updateFunc(new_values, prev_running_count):\n",
    "    return sum(new_values) + (prev_running_count or 0)\n",
    "  \n",
    "# Create a local StreamingContext with as many working processors as possible and a batch interval of 10 seconds            \n",
    "batch_interval = 10\n",
    "\n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[*]\", appName = \"WordCountApp\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "#! To enable checkpointing\n",
    "#! To store the state of the streaming application periodically to a reliable source\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "lines = ssc.socketTextStream(host, int(port))\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.updateStateByKey(updateFunc)\n",
    "#! updateStateByKey() allow to maintain arbitrary state while processing the incoming data stream\n",
    "#! the function is applied to EACH BATCH OF THE INPUT DATA STREAM to UPDATE THE STATE\n",
    "#! the UPDATED state is then stored in memory and can be used in the next batch processing\n",
    "#! Mostly for maintain and update a rolling count or cumulative count of data\n",
    "\n",
    "# Print the result                            \n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "try:\n",
    "    ssc.awaitTermination(timeout=60)\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "\n",
    "ssc.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoO3ir6BOEd0"
   },
   "source": [
    "Can you see the effect of using the `updateStateByKey` operation? YES it's function is obvious. This operation is calling a function (`updateFunc`).  The `updateFunc` function has two parameters: \n",
    "    1. `new_values` having a sequence of (word, 1) pairs \n",
    "    2. `prev_running_count` having the previous count information of the pairs. \n",
    "\n",
    "\n",
    "Note that the `updateStateByKey` opertion needs the checkpoint directory to be configured. \n",
    "\n",
    "##### Checkpointing\n",
    "\n",
    "A streaming application must run 24 hours a day. Thus, it needs to be resilient to failures caused by some unexpected errors such as system failures, driver failure, JVM crashes, etc. Checkpointing saves the generated RDDs to a reliable storate and performs receovery from an error. \n",
    "\n",
    "To summarise, checkpoints provide a way of recovering to a safe stable application snapshot. Using the `ssc.checkpoint()` method, we can tell the Spark engine **where to store the checkpoint files**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Options for checkpointing**\n",
    "- HDFS (Hadoop Distributed File System)\n",
    "    - Designed for storing large datasets across clusters of computers\n",
    "    - Parallel processing\n",
    "        - Access and store data at the same time\n",
    "    - Scalable\n",
    "        - U want more space? Add more nodes (computers)\n",
    "    - Fault tolerance\n",
    "        - Replicates data across multiple nodes, maybe 4 copies\n",
    "- Cloud Storage\n",
    "- Local:\n",
    "    - MUST AVOID IT AT ALL COST = SERIOUS PROBLEM --> DATA LOSS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMY3ltBBOEd0"
   },
   "source": [
    "### Window Operation\n",
    "Spark Streaming also provides windowed computations. This function allows to apply transformations over a sliding window of data. \n",
    "\n",
    "Every time the window slides over a source DStream. Thus, the source RDDs that fall within the window are combined and operated to produce the RDDs of the windowed DStream. \n",
    "\n",
    "A window operation needs two parameters:\n",
    "    1. window length: the duration of the window.\n",
    "    2. sliding interval: the interval at which the window operation is performed.\n",
    "\n",
    "These two parameters must be multiples of the batch interval (i.e. in our example: 10 sec) the source DStream.\n",
    "\n",
    "To illustrate, refer to our previous example. If we want to generate word counts over the last 20 seconds of data, every 10 seconds, we can use the following command:\n",
    "\n",
    "```\n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 20, 10)\n",
    "```\n",
    "\n",
    "```\n",
    "reduceByKeyAndWindow()\n",
    "```\n",
    "\n",
    "<font color='red'>\n",
    "- Takes in 3 arguements by default but we will USE 4\n",
    "    \n",
    "- 1st : Reduce function, to combine the count of the words over the window\n",
    "    \n",
    "- 2nd : (OPTIONAL) Inverse reduce function, to remove the counts of words that are no longer in the window\n",
    "    \n",
    "- 3rd : Window size/length (20 seconds)\n",
    "    \n",
    "- 4th : Sliding interval (10 seconds)\n",
    "</font><br>\n",
    "\n",
    "<font color='blue'>\n",
    "**Exercise**: Apply the reduceByKeyAndWindow operation, and check how it is working!\n",
    "</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "**WHAT IS A SLIDING WINDOW OF DATA**\n",
    "- Method to perform windowed computation on a continuous stream of data\n",
    "- Break down a continuous stream of data into discrete chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o10033.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:568)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:763)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:576)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m wordCounts\u001b[38;5;241m.\u001b[39mpprint()\n\u001b[1;32m     38\u001b[0m windowedWordCounts\u001b[38;5;241m.\u001b[39mpprint()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     ssc\u001b[38;5;241m.\u001b[39mawaitTermination(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/streaming/context.py:214\u001b[0m, in \u001b[0;36mStreamingContext.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    Start the execution of the streams.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     StreamingContext\u001b[38;5;241m.\u001b[39m_activeContext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o10033.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:568)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:763)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:576)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:21:40\n",
      "-------------------------------------------\n",
      "('line', 10)\n",
      "('0', 2)\n",
      "('1', 1)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 5)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:21:50\n",
      "-------------------------------------------\n",
      "('4', 1)\n",
      "('line', 10)\n",
      "('0', 1)\n",
      "('1', 4)\n",
      "('This', 10)\n",
      "('5', 2)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:22:00\n",
      "-------------------------------------------\n",
      "('4', 1)\n",
      "('line', 10)\n",
      "('1', 2)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('5', 2)\n",
      "('3', 3)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:22:10\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('0', 1)\n",
      "('1', 3)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:22:20\n",
      "-------------------------------------------\n",
      "('4', 1)\n",
      "('line', 10)\n",
      "('0', 3)\n",
      "('1', 3)\n",
      "('This', 10)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:22:30\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('1', 2)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('3', 3)\n",
      "('is', 10)\n",
      "('2', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:22:40\n",
      "-------------------------------------------\n",
      "('4', 3)\n",
      "('line', 10)\n",
      "('1', 1)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('5', 3)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:22:50\n",
      "-------------------------------------------\n",
      "('4', 4)\n",
      "('line', 10)\n",
      "('0', 2)\n",
      "('1', 1)\n",
      "('This', 10)\n",
      "('5', 2)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:23:00\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('0', 2)\n",
      "('1', 1)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:23:10\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('1', 2)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:23:20\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('1', 1)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('5', 2)\n",
      "('3', 3)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:23:30\n",
      "-------------------------------------------\n",
      "('4', 4)\n",
      "('line', 10)\n",
      "('0', 3)\n",
      "('1', 1)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:23:40\n",
      "-------------------------------------------\n",
      "('line', 10)\n",
      "('1', 2)\n",
      "('0', 3)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 3)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:23:50\n",
      "-------------------------------------------\n",
      "('4', 5)\n",
      "('line', 9)\n",
      "('1', 2)\n",
      "('This', 9)\n",
      "('3', 1)\n",
      "('is', 9)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:24:00\n",
      "-------------------------------------------\n",
      "('4', 4)\n",
      "('line', 10)\n",
      "('0', 1)\n",
      "('1', 1)\n",
      "('This', 10)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "('2', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:24:10\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('0', 2)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "('2', 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# add the new values with the previous running count to get the new count\n",
    "def updateFunc(new_values, prev_running_count):\n",
    "    return sum(new_values) + (prev_running_count or 0)\n",
    "  \n",
    "# Create a local StreamingContext with as many working processors as possible and a batch interval of 10 seconds            \n",
    "batch_interval = 10\n",
    "\n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[*]\", appName = \"WordCountApp\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "#! To enable checkpointing\n",
    "#! To store the state of the streaming application periodically to a reliable source\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "lines = ssc.socketTextStream(host, int(port))\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.updateStateByKey(updateFunc)\n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x+y, lambda x, y: x-y, 20, 10)\n",
    "\n",
    "# Print the result                            \n",
    "wordCounts.pprint()\n",
    "windowedWordCounts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "try:\n",
    "    ssc.awaitTermination(timeout=60)\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "\n",
    "ssc.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OpZIehxXOEd2"
   },
   "source": [
    "### Join Operations\n",
    "\n",
    "Also, we can easily join two different streams into one stream data in Spark Streaming.\n",
    "\n",
    "For example, if we want to join the `stream2` data into the `stream1` data, we can use the following code: \n",
    "\n",
    "```\n",
    "stream1 = ...\n",
    "stream2 = ...\n",
    "joinedStream = stream1.join(stream2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_b38urMOEd3"
   },
   "source": [
    "### Output Operations on DStreams ##\n",
    "\n",
    "When we want to send DStream to an external system or database, we can use various output operations. The following output operations can be used:\n",
    "\n",
    "    - print(): print the first ten elements of every batch of data in a DStream running the streaming application. In Python, pprint() corresponds to print().\n",
    "    - saveAsTextFiles(prefix, [suffix]): save the DStream data as text files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n",
    "    - foreachRDD(func): Each RDD in DStream can be pushed out using this method. Note that the function `func` is executed on the running the streaming application, and will usually have RDD actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNrzDEqcOEd4"
   },
   "source": [
    "For example, with our original example, on the `wordCounts` DStream, we can use the following code:\n",
    "\n",
    "```\n",
    "def sendPartition(iter):\n",
    "    connection = createNewConnection() # Assuming such fucntion exists\n",
    "    for record in iter:\n",
    "        connection.send(record)\n",
    "    connection.close()\n",
    "    \n",
    "wordCounts.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n",
    "```\n",
    "\n",
    "In `sendPartition()`, we create a single connection object and send all the records in a RDD partition using that connection.\n",
    "\n",
    "As an example, if we can store each RDD into a MongoDB database, for example the `test_db`, then we can use the following code in the `sendPartition()` function:\n",
    "\n",
    "```\n",
    "connection = MongoClient()\n",
    "test_db = connection.get_database('test_db')\n",
    "....\n",
    "```\n",
    "You will learn more on this topic in next tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spark Structured Streaming\n",
    "\n",
    "Reference link: <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">[REF]</a>\n",
    "\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. This is a simple example of a Structured Streaming query adapted from Spark's official documentation.\n",
    "\n",
    "Let’s say you want to maintain a running word count of text data received from a data server listening on a TCP socket. Make sure the TCP Server notebook is running. \n",
    "\n",
    "First, we have to import the necessary classes and create a local SparkSession, the starting point of all functionalities related to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark streaming in based on micro-batch processing. Data is processed in small and discrete batches\n",
    "- Structured streaming is based on continuous processing. Data is processed continuously as it arrived\n",
    "- Structured streaming is generally considered easier to use than Spark streaming\n",
    "- Structured streaming is introduced in Spark 2.0 where spark streaming is Spark 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create a streaming DataFrame that represents text data received from a server listening on localhost:9999, and transform the DataFrame to calculate word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_sdf = (\n",
    "    spark.readStream\n",
    "    .format('socket')\n",
    "    .options(host='localhost', port=9999)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_sdf = (\n",
    "    socket_sdf\n",
    "    #! Create a new column / replace an existing column\n",
    "    .withColumn('value', split('value', '\\s+'))\n",
    "    #! select a column from the dataframe\n",
    "    .select( explode('value').alias('word') ) \n",
    "    # In this case we select the 'word' column by exploding the 'value' column using explode()\n",
    "    # explode() used to transform array-like column into a set of rows,\n",
    "    # with each row containing 1 value from the array\n",
    "    .groupBy('word')\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **word_count_sdf** DataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have used two built-in SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we use the function alias to name the new column as “word”. Finally, we group the unique values in the Dataset and count them. Note that this is a streaming DataFrame which represents the running word counts of the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = (\n",
    "    word_counts_sdf.writeStream\n",
    "    .format('console')\n",
    "    .outputMode('complete')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the code above is executed, the streaming computation will have started in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:56:40\n",
      "-------------------------------------------\n",
      "('4', 4)\n",
      "('line', 10)\n",
      "('1', 1)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('5', 2)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:56:50\n",
      "-------------------------------------------\n",
      "('4', 4)\n",
      "('line', 10)\n",
      "('0', 3)\n",
      "('This', 10)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:57:00\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('1', 1)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "('2', 4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:57:10\n",
      "-------------------------------------------\n",
      "('4', 4)\n",
      "('line', 10)\n",
      "('0', 3)\n",
      "('This', 10)\n",
      "('is', 10)\n",
      "('2', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:57:20\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('0', 2)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "('2', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:57:30\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('1', 3)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "('2', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:57:40\n",
      "-------------------------------------------\n",
      "('4', 3)\n",
      "('line', 10)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('5', 2)\n",
      "('is', 10)\n",
      "('2', 4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:57:50\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('1', 3)\n",
      "('0', 2)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:58:00\n",
      "-------------------------------------------\n",
      "('4', 1)\n",
      "('line', 10)\n",
      "('1', 1)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('5', 1)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 5)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:58:10\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('1', 2)\n",
      "('0', 1)\n",
      "('This', 10)\n",
      "('3', 1)\n",
      "('is', 10)\n",
      "('2', 4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:58:20\n",
      "-------------------------------------------\n",
      "('4', 2)\n",
      "('line', 10)\n",
      "('0', 1)\n",
      "('1', 2)\n",
      "('This', 10)\n",
      "('5', 2)\n",
      "('3', 2)\n",
      "('is', 10)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:58:30\n",
      "-------------------------------------------\n",
      "('4', 1)\n",
      "('line', 9)\n",
      "('1', 1)\n",
      "('0', 2)\n",
      "('This', 9)\n",
      "('5', 1)\n",
      "('3', 3)\n",
      "('is', 9)\n",
      "('2', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:58:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:58:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:59:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:59:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:59:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:59:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:59:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 03:59:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:00:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:00:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:00:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:00:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:00:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:00:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:01:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:01:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:01:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:01:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:01:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:01:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:02:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:02:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:02:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:02:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:02:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:02:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:03:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:03:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:03:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:03:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:03:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:03:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:04:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:04:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:04:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:04:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:04:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:04:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-30 04:05:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6S_xyisqOEd5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations on finishing this activity!\n",
    "\n",
    "<font color='blue'>\n",
    "**Wrap up what we've learned:**\n",
    "- Learned that Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data.\n",
    "- Learned that Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results\n",
    "- Learned that using \"StreamingContext\", we can define the input sources by creating input DStreams; apply transformation and output operations to DStreams; and receive data and process it.\n",
    "- Learned that the \"updateStateByKey\" operation allows you to maintain arbitrary state while continuously updating it with new information. \n",
    "- Learned how to use \"dstream.foreachRDD\" that allows data to be sent out to external systems.\n",
    "    \n",
    "**Note: From Spark 3 onwards, the DStream API for Kafka integration has been removed for Python. Therefore, in this tutorial, we also introduced you to Structured Streaming with Spark. With structured streaming, you can express your streaming computation the same way you would express a batch computation on static data. You will use structured streaming in your assignment.**\n",
    "    \n",
    "Extra reading: Do refer to this paper which dives deeper into Strcutured Streaming with SPARK:\n",
    "    \n",
    "https://dl-acm-org.ezproxy.lib.monash.edu.au/doi/pdf/10.1145/3183713.3190664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5148 - Sparking_Streaming_With_Python.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
