{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 : Streaming data from open data\n",
    "\n",
    "This week's code is about getting parking sensor data from a Kafka topic and storing that in MongoDB for further processing. The code is more or less the same as Week 10, with the exception of the `DbWriter` class that has been modified to do some preprocessing on the streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ip = \"10.192.45.93\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Streaming Parking Data')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'week11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't care about any columns other than `value`, which contains our serialized (JSON) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_sdf = kafka_sdf.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbWriter:\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{host_ip}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.mongo_client['fit3182_db']\n",
    "        return True\n",
    "    \n",
    "    # called once per row of the result dataframe\n",
    "    # the current code DOES NOT handle duplicate processing\n",
    "    #   e.g., query fails and restarts just before current micro-batch was fully inserted\n",
    "    def process(self, row):\n",
    "        data = json.loads(row.value)\n",
    "        \n",
    "        db_record = {}\n",
    "        db_record['_id'] = data.get('st_marker_id')\n",
    "        db_record['latitude'] = data.get('lat')\n",
    "        db_record['longitude'] = data.get('lon')\n",
    "        db_record['status'] = data.get('status')\n",
    "        \n",
    "        self.db[topic].replace_one({'_id': data.get('st_marker_id')}, db_record, upsert=True)\n",
    "    \n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging, print on console\n",
    "class ConsoleWriter:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        return True\n",
    "    \n",
    "    def process(self, row):\n",
    "        data = json.loads(row.value)\n",
    "        \n",
    "        record = {}\n",
    "        record['_id'] = data.get('st_marker_id')\n",
    "        record['latitude'] = data.get('lat')\n",
    "        record['longitude'] = data.get('lon')\n",
    "        record['status'] = data.get('status')\n",
    "        print(record)\n",
    "        \n",
    "    def close(self, err):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = (\n",
    "    parking_sdf.writeStream.format(\"Console\")\n",
    "    .option(\"checkpointLocation\", \"./parking_sdf_checkpoints\")\n",
    "    .outputMode('append').foreach(DbWriter())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
