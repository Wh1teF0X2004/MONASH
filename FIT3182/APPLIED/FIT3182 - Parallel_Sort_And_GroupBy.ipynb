{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5VW0TzHdR_"
   },
   "source": [
    "# FIT3182 - Big data management and processing\n",
    "\n",
    "# Activity: Parallel Sort and GroupBy#\n",
    "\n",
    "This activity consists of two parts. In the first part, we will learn and build different **serial/parallel sorting algorithms** where the volume of data to be sorted is large and stored in a database. In the second part, we focus on implementing **serial/parallel GroupBy** queries. GroupBy queries involving aggregates are very common in database processing, especially in Online Analytical Processing (OLAP), and data warehouse.\n",
    "\n",
    "This activity will help you to learn how parallel sorting and GroupBy operations can be implemented for parallel database systems.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Read the code base and comments carefully\n",
    "- After understanding the provided function, run the cell right below it to check if the result is correct.\n",
    "- Read carefully all the **Exercise** tasks below. There are some code blocks that **you need to complete** yourself.\n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to build serial/parallel sorting algorithms\n",
    "- Be able to build serial/parallel groupby operations\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ob4tRk5GHdSA"
   },
   "source": [
    "<font color='blue'>\n",
    "**What you need to remember**:\n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRWxd1YrHdSB"
   },
   "source": [
    "### Dataset ###\n",
    "In this activity, we use the following dataset R consisting of numbers for simplicity. In the real world, each number indicates a record. R indicates our experimental entire record set that contains unordered numbers ranging from 1 to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImhBOqgmHdSC"
   },
   "outputs": [],
   "source": [
    "R = [8, 12, 16, 4, 11, 15, 3, 7, 14, 2, 6, 10, 1, 5, 9, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CP659tbfHdSF"
   },
   "source": [
    "## Quicksort Algorithm ##\n",
    "\n",
    "Throughout this activity, as an internal sorting method, we will use the quicksort method. In internal sorting, sorting takes place totally within main memory. The data to be sorted is assumed to be small and fits into main memory. This sorting method will be commonly used in the serial/parallel external sorting methods below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">\n",
    "\n",
    "### PROS:\n",
    "- Fast, for small table (Average time complexity: O(n log(n))\n",
    "- IN PLACE SORT --> does not require temporary memory\n",
    "\n",
    "### CONS\n",
    "- VERY VERY VERY SLOW in worst case (Time complexity: O(n^2))\n",
    "- Not Stable\n",
    "\n",
    "### STABILITY\n",
    "- Our data = (4, 'first'), (2, 'AB'), (3, 'AC'), (4, 'second'), (1, 'AA')\n",
    "- If we compare only the int, ignoring str, we choose 3 as pivot\n",
    "- What is stability? After the sort, the 2 elements that compare as if equal (eg. the two 4s) appear in the same order afterwards as they did before. IT PRESERVES THE RELATIVE ORDER\n",
    "- STABLE SORT: (1, 'AA'), (2, 'AB'), (3, 'AC'), (4, 'first'), (4, 'second')\n",
    "- UNSTABLE SORT: (1, 'AA'), (2, 'AB'), (3, 'AC'), (4, 'second'), (4, 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqlA6wKLHdSF"
   },
   "outputs": [],
   "source": [
    "def qsort(arr): \n",
    "\n",
    "    \"\"\" \n",
    "    Quicksort a list\n",
    "    \n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        #take the first element as the pivot\n",
    "        pivot = arr[0]\n",
    "        left_arr = [x for x in arr[1:] if x < pivot] # WHATEVER SMALLER THAN THE PIVOT\n",
    "        right_arr = [x for x in arr[1:] if x >= pivot] # WHATEVER LARGER THAN THE PIVOT\n",
    "        # uncomment this to see what to print \n",
    "        # print(\"Left:\" + str(left_arr)+\" Pivot : \"+ str(pivot)+\" Right: \" + str(right_arr))\n",
    "        value = qsort(left_arr) + [pivot] + qsort(right_arr)\n",
    "        \n",
    "        return value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOVMzUs6HdSJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left:[4, 3, 7, 2, 6, 1, 5] Pivot : 8 Right: [12, 16, 11, 15, 14, 10, 9, 13]\n",
      "Left:[3, 2, 1] Pivot : 4 Right: [7, 6, 5]\n",
      "Left:[2, 1] Pivot : 3 Right: []\n",
      "Left:[1] Pivot : 2 Right: []\n",
      "Left:[6, 5] Pivot : 7 Right: []\n",
      "Left:[5] Pivot : 6 Right: []\n",
      "Left:[11, 10, 9] Pivot : 12 Right: [16, 15, 14, 13]\n",
      "Left:[10, 9] Pivot : 11 Right: []\n",
      "Left:[9] Pivot : 10 Right: []\n",
      "Left:[15, 14, 13] Pivot : 16 Right: []\n",
      "Left:[14, 13] Pivot : 15 Right: []\n",
      "Left:[13] Pivot : 14 Right: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qsort(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzeVVFf5HdSO"
   },
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr align=\"left\">\n",
    "     <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c0Ze4bmHdSP"
   },
   "source": [
    "## 1. Serial External Sorting based on Sort-Merge ##\n",
    "\n",
    "The serial sorting method we consider is **serial external sorting** which is external sorting in a uniprocessor environment. The most common serial external sorting algorithm is based on **sort-merge**. The underlying idea is that we (1) break the given record set into unsorted sub-record sets, (2) sort the sub-record sets, and (3) merge them into larger and larger sorted sub-record sets until the entire record set is sorted. In the real-word, **each sub-record set** is replaced by **a file**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kh_SX4mIHdSP"
   },
   "source": [
    "### Note ###\n",
    "It is important to determine the size of each sub-record set to be sorted. Each sub-record set must be small enough to fit into the main memory. The size of these sub-record sets is determined by the **buffer size in main memory**, which is to be used for sorting each sub-record set internally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"maroon\">\n",
    "\n",
    "### STEPS:\n",
    "    1. Divide the input dataset into smaller chunks that can fit into memory\n",
    "    2. Sort each chunk in memory\n",
    "    3. Write sorted chunks into disk\n",
    "    4. Merge sorted chunks\n",
    "    5. Write final sorted output to disk\n",
    "    6. Repeat step 1-5 until entire dataset is sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"maroon\">\n",
    "    \n",
    "### EXTRA NOTES:\n",
    "\n",
    "- K-way merge always compare the first element of each list\n",
    "- Min-array should be initialised to keep all the min element into the array (to keep track)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"maroon\">\n",
    "\n",
    "1. **INITIALISATION**:\n",
    "- Create a result array or list to store the final output\n",
    "- Create a min-heap to efficiently select the minimum element among the heads of the k input arrays\n",
    "\n",
    "2. **LOAD THE INITIAL ELEMENTS**\n",
    "- Load the first element from each of the k input array into the min-heap along with the index of the array it belongs to\n",
    "\n",
    "3. **MERGING PROCESS**\n",
    "- While the min-heap is not empty:\n",
    "    - Pop the smallest elemnet frim the min-heap\n",
    "    - Add this element to the result array\n",
    "    - Load the next element from the same array as the popped element into the min-heap\n",
    "    - If array is exhausted, ignore it and continue with other arrays\n",
    "    - Repeat this process until all elements from all input arrays are processed\n",
    "    \n",
    "4. **FINIALIZATION**\n",
    "- Once all elements are processed, the result array contains the merged output of all k input arrays\n",
    "\n",
    "**TIME COMPLEXITY ANALYSIS**\n",
    "- Loading the initial elements into the min-heap takes O(k) time\n",
    "- Each iteration for the merge process involves extracting the minimum elements from the min-heap, it takes O(log(k)) time\n",
    "- Since we have a total of n elements acorss all k input arrays, the merge process required O(n log(k)) time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TaLZrbkuHdSQ"
   },
   "source": [
    "**Exercise**: Understand and run the following serial external sorting algorithm. Then, discuss the time complexity of this algorithm as well as its pros and cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ-m01kcHdSQ"
   },
   "outputs": [],
   "source": [
    "# Let's first look at 'k-way merging algorithm' that will be used \n",
    "# to merge sub-record sets in our external sorting algorithm.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if(records[i] < m):  \n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []  \n",
    "    \n",
    "    # the merging unit (i.e. # of the given buffers)\n",
    "    tuple = []\n",
    "    \n",
    "    while(True):\n",
    "        tuple = [] # initialise tuple\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                # WE REACH THE END, SUB-LIST IS EMPTY\n",
    "                # SYS.MAXSIZE IS A CONSTANT THAT REPRESENTS THE MAX SIZE THAT A VARIABLE CAN TAKE ON BASED ON UR PLATFORM, PROCESSOR, OS\n",
    "                tuple.append(sys.maxsize)\n",
    "            else:\n",
    "                # OTHERWISE, ADD THE CURRENT RECORD FROM THE BUFFER TO THE TUPLE\n",
    "                tuple.append(record_sets[i][indexes[i]])  \n",
    "        \n",
    "        # find the smallest record \n",
    "        # THIS IS THE MIN-HEAP\n",
    "        smallest = find_min(tuple)\n",
    "    \n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(tuple[smallest] == sys.maxsize):\n",
    "            # REACH THE END OF EVERY RECORD THEN BREAK\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        # APPEND THE SMALLEST RECORD TO THE MERGED RESULT\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        # MOVE TO THE NEXT RECORD IN THE BUFFER FROM WHICH THE SMALLEST RECORD WAS TAKEN\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LndPoF8HdSU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "# Test k-way merging method\n",
    "buffers = [[1, 2, 3, 4, 8, 13], [5, 6, 7, 11, 12], [9, 10, 14, 15, 16]]\n",
    "result = k_way_merge(buffers)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6smYbqIHdSW"
   },
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr align=\"left\">\n",
    "     <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Complete the following serial external sorting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zx3ovUGAHdSY"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of eac sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # --- Sort Phase ---\n",
    "    sorted_set = []\n",
    "    \n",
    "    # Read buffer_size pages at a time into memory and\n",
    "    # sort them, and write out a sub-record set (i.e. variable: subset)\n",
    "    start_position = 0\n",
    "    n = len(dataset) # Length of dataset\n",
    "    while True:\n",
    "        # IF THE NO. OF RECORDS REMAINING IS THE DATASET IS GREATER THAN THE BUFFER SIZE\n",
    "        if ((n - start_position) > buffer_size):\n",
    "            # B = Buffer size\n",
    "            # Read B-records from the input\n",
    "            # Ending position --> start_position + buffer_size\n",
    "            subset = dataset[start_position : start_position + buffer_size]\n",
    "            # Sort the subset using quicksort\n",
    "            sorted_subset = qsort(subset)\n",
    "            sorted_set.append(sorted_subset)\n",
    "            # Update starting position to the next position\n",
    "            start_position += buffer_size\n",
    "        else:\n",
    "            # read last B-records from the input, where B is less than buffer_size\n",
    "            # from start_position to the end\n",
    "            subset = dataset[start_position:]\n",
    "            # Sort the subset using quicksort\n",
    "            sorted_subset = qsort(subset)\n",
    "            sorted_set.append(sorted_subset)\n",
    "            break # Break because we stop when the entire dataset has been processed\n",
    "    \n",
    "    # --- Merge Phase ---\n",
    "    # We need to -1 from the buffer size. WHY?\n",
    "    # To ensure that they can fit into the memory\n",
    "    # Last buffer is used to store the temporary output\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    # Output similar to --> [[4, 8, 12, 16],[3, 7, 11, 15],[2, 6, 10, 14],[1, 5, 9, 13]]\n",
    "    dataset = sorted_set\n",
    "    # NOW YOU HAVE EVERYTHING SORTED BUT NOW YOU WANT TO MERGE THEM TGT\n",
    "    while True:\n",
    "        merged_set = []\n",
    "        n = len(dataset)\n",
    "        start_position = 0\n",
    "        while True:\n",
    "            if ((n - start_position) > merge_buffer_size):\n",
    "                # Read C-record sets from the merged record sets, where C = merged_buffer_size\n",
    "                # [4, 8, 12, 16],[3, 7, 11, 15],[2, 6, 10, 14]\n",
    "                subset = dataset[start_position: start_position + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset)) # merge list in the subset\n",
    "                # The merged_set only contain 3 not, not 4, [1, 5, 9, 13] not yet included\n",
    "                start_position += merge_buffer_size\n",
    "            else:\n",
    "                subset = dataset[start_position:]\n",
    "                merged_set.append(k_way_merge(subset)) # You merge the list in the subset\n",
    "                break # Break cus there's ntg else as its the last record\n",
    "                \n",
    "        # Print end result\n",
    "        print(\"INTERMEDIATE MERGED SET\", merged_set)\n",
    "        \n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1):\n",
    "            result = merged_set\n",
    "            break\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGPKSnMhHdSa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left:[4] Pivot : 8 Right: [12, 16]\n",
      "Left:[] Pivot : 12 Right: [16]\n",
      "Left:[3, 7] Pivot : 11 Right: [15]\n",
      "Left:[] Pivot : 3 Right: [7]\n",
      "Left:[2, 6, 10] Pivot : 14 Right: []\n",
      "Left:[] Pivot : 2 Right: [6, 10]\n",
      "Left:[] Pivot : 6 Right: [10]\n",
      "Left:[] Pivot : 1 Right: [5, 9, 13]\n",
      "Left:[] Pivot : 5 Right: [9, 13]\n",
      "Left:[] Pivot : 9 Right: [13]\n",
      "INTERMEDIATE MERGED SET [[2, 3, 4, 6, 7, 8, 10, 11, 12, 14, 15, 16], [1, 5, 9, 13]]\n",
      "INTERMEDIATE MERGED SET [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n",
      "final sorting result:[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "result = serial_sorting(R, 4)\n",
    "print(\"final sorting result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qh2co5KjHdSe"
   },
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr align=\"left\">\n",
    "     <td>final sorting result:[[1, 2, 3, 5, 6, 4, 7, 9, 10, 8, 11, 13, 14, 12, 15, 16]]</td> \n",
    "    </tr>\n",
    " \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyJJwc3lHdSf"
   },
   "source": [
    "## 2.  Algorithms for Parallel External Sort ##\n",
    "Having practiced how serial external sorting works, let's move onto building parallel sorting methods. In the lectures, you have learned a number of different parallel sorting methods. For this activity, we focus on: **Parallel Merge-All Sort**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lT9I3G9pHdSg"
   },
   "source": [
    "### 2.1 Parallel Merge-All Sort ###\n",
    "The Parallel merge-all sort method is a traditional approach and is composed of two phases: **(1) local sort** and **(2) final merge**. The first phase is carried out independently in each processor. Local sorting in each processor is performed as per a normal serial external sorting mechanism. In the final merge phase, the results from the local sort phase are merged. The final merge phase is carried out by one processor, namely, the host using k-way mergin (see function `k_way_merge()` above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEXjf6RbHdSh"
   },
   "source": [
    "#### Use the round-robin data partitioning method ####\n",
    "As a pre-requiste process, we first need to partition the given data into a number of subsets according to the number of parallel processors available. As mentioned above, let's assume that we use the round-robin partitioning method. Refer to the \"Parallel Search\" activity and copy the `rr_partition()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYIA1WN6HdSh"
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        result[index_bin].append(element)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EjvOIBQeHdSk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 11, 14, 1], [12, 15, 2, 5], [16, 3, 6, 9], [4, 7, 10, 13]]\n"
     ]
    }
   ],
   "source": [
    "# Test the round-robin partitioning function\n",
    "result = rr_partition(R, 4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcEgp0rBHdSp"
   },
   "source": [
    "**Expected Output**: \n",
    "<table align='center'>\n",
    "    <tr>\n",
    "     <td>[[8, 11, 14, 1], [12, 15, 2, 5], [16, 3, 6, 9], [4, 7, 10, 13]]</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZ4_hs9dHdSq"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge-all sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    sorted_set = []\n",
    "    for s in subsets:\n",
    "        # call the serial_sorting method above\n",
    "        sorted_set.append(*pool.apply_async(serial_sorting, [s, buffer_size]).get())\n",
    "        # * = unpacking --> is like [[]] become []\n",
    "    pool.close()\n",
    "    \n",
    "    # ---- Final merge phase ----\n",
    "    print(\"sorted entire set:\" + str(sorted_set))\n",
    "    result = k_way_merge(sorted_set)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lilp0rfPHdSu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left:[1] Pivot : 8 Right: [11, 14]\n",
      "Left:[] Pivot : 11 Right: [14]\n",
      "INTERMEDIATE MERGED SET [[1, 8, 11, 14]]\n",
      "Left:[2, 5] Pivot : 12 Right: [15]\n",
      "Left:[] Pivot : 2 Right: [5]\n",
      "INTERMEDIATE MERGED SET [[2, 5, 12, 15]]\n",
      "Left:[3, 6, 9] Pivot : 16 Right: []\n",
      "Left:[] Pivot : 3 Right: [6, 9]\n",
      "Left:[] Pivot : 6 Right: [9]\n",
      "INTERMEDIATE MERGED SET [[3, 6, 9, 16]]\n",
      "Left:[] Pivot : 4 Right: [7, 10, 13]\n",
      "Left:[] Pivot : 7 Right: [10, 13]\n",
      "Left:[] Pivot : 10 Right: [13]\n",
      "INTERMEDIATE MERGED SET [[4, 7, 10, 13]]\n",
      "sorted entire set:[[1, 8, 11, 14], [2, 5, 12, 15], [3, 6, 9, 16], [4, 7, 10, 13]]\n",
      "final result:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_merge_all_sorting(R, 4, 4)\n",
    "print(\"final result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mjv6f_FdHdSy"
   },
   "source": [
    "**Expected Output**: \n",
    "<table aligh='left'>\n",
    "    <tr><td>sorted entire set:[[1, 8, 11, 14], [2, 5, 12, 15], [3, 6, 9, 16], [4, 7, 10, 13]]</td></tr>\n",
    "    <tr><td>final result:[1, 2, 3, 4, 5, 6, 7, 9, 10, 13, 8, 11, 12, 14, 15, 16]</td>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"maroon\">\n",
    "    \n",
    "### PROS:\n",
    "- Parallel-merge all sort has a time complexity of O(n log(n)) which is the same as merge-sort algorithm\n",
    "- This is a parallel algorithm, it can take advantages of using multi-core processors to sort large datasets faster than a single-threaded algorithm\n",
    "- It can handle datasets that are too large to fit into memory, by dividing the sorting process into smaller chunks that can fit into the memory\n",
    "\n",
    "### CONS:\n",
    "- Global aggregation is carried out by one processor. No parallelism in the global aggregation phase, results in bottleneck\n",
    "- If you are using multiple machines across the network, network bottleneck when sending the local sorting results to the coordinator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8iOyBmTHdTB"
   },
   "source": [
    "## 3. Parallel Algorithms for GroupBy Queries ##\n",
    "\n",
    "Parallel aggregate processing is very similar to parallel sorting. From the lessons we learned from parallel sorting, we focus on one parallel aggregate query algorithms: **A traditional merge-all method**.\n",
    "\n",
    "- GroupBy queries are commonly used in databased to aggregate data based on specific criteria\n",
    "- For example: calculating average age of employees in each department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-FdqFmVHdTB"
   },
   "source": [
    "## 3.1 Merge-All GroupBy Method ###\n",
    "This method takes two phases: (1) a local aggregation step, and (2) a global aggregation step.\n",
    "In the first step, each processor groups local records according to the designated group-by attribute and performs the aggregate function.  The second step is a global aggregation step, in which all the temporary results obtained in each node are passed to the host for consolidation in order to produce the global aggregate values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22PGYCp7HdTB"
   },
   "source": [
    "### The dataset for the GroupBy implementation ###\n",
    "Let's assume that we have two different datasets `D1` and `D2` where each dataset will be handled by a processor in a local aggregation step. In the second global aggregation step, the aggreated results will be handled by the host. Each record is represented by a nominal key and a numeric value. Note that duplicated keys exist in `D1` and `D2`. For our GroupBy implementation, we retrieve pairs of keys and values according to the key attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MndY4sjlHdTC"
   },
   "outputs": [],
   "source": [
    "D1 = [('A', 1), ('B', 2), ('C', 3), ('A', 10), ('B', 20), ('C', 30)]\n",
    "D2 = [('A', 4), ('B', 5), ('C', 6), ('A', 40), ('B', 50), ('C', 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSZXEm7YHdTE"
   },
   "source": [
    "**Exercise**: Understand and run the fist phase of the parallel merge-all GroupBy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqCeh-SWHdTE"
   },
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "def local_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = 0\n",
    "        # SUM IT UP\n",
    "        dict[key] += val\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by8sApqkHdTG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 11, 'B': 22, 'C': 33}\n",
      "{'A': 44, 'B': 55, 'C': 66}\n"
     ]
    }
   ],
   "source": [
    "result = local_groupby (D1)\n",
    "print(result)\n",
    "result = local_groupby (D2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kF1QkzpHdTI"
   },
   "source": [
    "**Expected Output**: \n",
    "<table aligh='left'>\n",
    "    <tr><td>{'A': 11, 'B': 22, 'C': 33}</td></tr>\n",
    "    <tr><td>{'A': 44, 'B': 55, 'C': 66}</td></tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6B5cBYyHdTJ"
   },
   "source": [
    "**Exercise**: **Complete the parallel merge-all groupby algorithm** by implementing the following code block between '### START CODE HERE ###' and '### END CODE HERE ###'. You need to use the local aggregation method defined above (i.e. **local_groupby()**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"maroon\">\n",
    "    \n",
    "1. **GROUPING PHASE**:\n",
    "    - Divide the dataset into groups based on a key\n",
    "    - Each group contains elemenst with the same key value\n",
    "2. **SORTING PHASE**:\n",
    "    - Sort each group individually using a sorting algorithm\n",
    "    -  Parallelize the sorting process for each group if possible\n",
    "3. **MERGE PHASE**:\n",
    "    - Merge all groups at the same time to produce the final sorting output\n",
    "    - Use a merge algorithm to merge elements from all groups while maintaining the sorted order\n",
    "    \n",
    "- \"Merge GroupBy\" is effective for sorting large dataset effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idSg0zb4HdTK"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_merge_all_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(dataset)\n",
    "\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in dataset:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby, [s]))\n",
    "    pool.close()\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    # Let's assume that the global operator is sum.\n",
    "    # Implement here\n",
    "    # local_result is after local_groupby\n",
    "    for r in local_result:\n",
    "        for key, val in r.items():\n",
    "            if key not in result:\n",
    "                result[key] = 0\n",
    "            result[key] += val\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fEgSlxsHdTL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 55, 'B': 77, 'C': 99}\n"
     ]
    }
   ],
   "source": [
    "E = [D1, D2]\n",
    "result = parallel_merge_all_groupby (E)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imkSrbm-HdTO"
   },
   "source": [
    "**Expected Output**: \n",
    "<table aligh='left'>\n",
    "    <tr><td>{'A': 55, 'B': 77, 'C': 99}</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1ExOjAsHdTP"
   },
   "source": [
    "Congratulations on finishing this activity!\n",
    "\n",
    "<font color='blue'>\n",
    "**Wrap up what we've learned:**\n",
    "- Internal sorting takes place totally within main memory. The data\n",
    "to be sorted is assumed to be small and fits into main memory. External sorting on the other hand is where the volume of data to be sorted is large and resides in secondary memory. Thus external sorting is also known as file sorting.\n",
    "- We practiced that how serial external sorting can be implemented using the k-way merge operation.\n",
    "- We are now able to build parallel external sorting methods using the serial external sorting methods: (1) parallel merge-all sort, and (2) parallel binary-merge sort\n",
    "- We now now able to build a parallel groubby method consisting of two phases: (1) a local aggregation step, and (2) a global aggregation step."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5148 - Parallel_Sort_And_GroupBy.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
