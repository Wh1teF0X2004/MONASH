{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed893fad",
   "metadata": {},
   "source": [
    "# Explore Feature Selection/Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcc0e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for the sake of readability \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096b351",
   "metadata": {},
   "source": [
    "The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.\n",
    "\n",
    "Irrelevant or partially relevant features can negatively impact model performance.\n",
    "\n",
    "In this notebook you will discover automatic feature selection techniques that you can use to prepare your machine learning data in python with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce44890",
   "metadata": {},
   "source": [
    "# Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a6d08",
   "metadata": {},
   "source": [
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested. Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfd245",
   "metadata": {},
   "source": [
    "Three benefits of performing feature selection before modeling your data are:\n",
    "\n",
    "* Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "\n",
    "* Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "\n",
    "* Reduces Training Time: Less data means that algorithms train faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40e4ee",
   "metadata": {},
   "source": [
    "This section will introduce you to a few options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7dbb2f",
   "metadata": {},
   "source": [
    "## 1. Select K Best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca977ad",
   "metadata": {},
   "source": [
    "## Select K best for Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02226a",
   "metadata": {},
   "source": [
    "\n",
    "The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features. (reference to https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0df51",
   "metadata": {},
   "source": [
    "### Step 1 : Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8156e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Univariate Statistical Tests\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_regression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_iris\n",
    "from numpy import array "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08915b24",
   "metadata": {},
   "source": [
    "### Step 2 : Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2776a6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature data dimension:  (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    " \n",
    "print(\"Feature data dimension: \", x.shape) \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca234bb",
   "metadata": {},
   "source": [
    "### Step 3: Execute the Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d73cde",
   "metadata": {},
   "source": [
    "   we'll define the model by using SelectKBest class. For classification we'll set 'chi2'  method as a scoring function. The target number of features is defined by k parameter. Then we'll fit and transform method on training x and y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ca1659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After selecting best 3 features: (150, 3)\n"
     ]
    }
   ],
   "source": [
    "select = SelectKBest(score_func=chi2, k=3)\n",
    "z = select.fit_transform(x,y)\n",
    " \n",
    "print(\"After selecting best 3 features:\", z.shape) \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2ac07",
   "metadata": {},
   "source": [
    "We've selected 3 best features in x data. To identify the selected features we use get_support() function and filter out them from the features name list.  The z object contains selected x data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48acd3fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features:\n",
      "['sepal length (cm)' 'sepal width (cm)' 'petal length (cm)'\n",
      " 'petal width (cm)']\n",
      "Selected best 3:\n",
      "['sepal length (cm)' 'petal length (cm)' 'petal width (cm)']\n",
      "[[5.1 1.4 0.2]\n",
      " [4.9 1.4 0.2]\n",
      " [4.7 1.3 0.2]\n",
      " [4.6 1.5 0.2]\n",
      " [5.  1.4 0.2]\n",
      " [5.4 1.7 0.4]\n",
      " [4.6 1.4 0.3]\n",
      " [5.  1.5 0.2]\n",
      " [4.4 1.4 0.2]\n",
      " [4.9 1.5 0.1]\n",
      " [5.4 1.5 0.2]\n",
      " [4.8 1.6 0.2]\n",
      " [4.8 1.4 0.1]\n",
      " [4.3 1.1 0.1]\n",
      " [5.8 1.2 0.2]\n",
      " [5.7 1.5 0.4]\n",
      " [5.4 1.3 0.4]\n",
      " [5.1 1.4 0.3]\n",
      " [5.7 1.7 0.3]\n",
      " [5.1 1.5 0.3]\n",
      " [5.4 1.7 0.2]\n",
      " [5.1 1.5 0.4]\n",
      " [4.6 1.  0.2]\n",
      " [5.1 1.7 0.5]\n",
      " [4.8 1.9 0.2]\n",
      " [5.  1.6 0.2]\n",
      " [5.  1.6 0.4]\n",
      " [5.2 1.5 0.2]\n",
      " [5.2 1.4 0.2]\n",
      " [4.7 1.6 0.2]\n",
      " [4.8 1.6 0.2]\n",
      " [5.4 1.5 0.4]\n",
      " [5.2 1.5 0.1]\n",
      " [5.5 1.4 0.2]\n",
      " [4.9 1.5 0.2]\n",
      " [5.  1.2 0.2]\n",
      " [5.5 1.3 0.2]\n",
      " [4.9 1.4 0.1]\n",
      " [4.4 1.3 0.2]\n",
      " [5.1 1.5 0.2]\n",
      " [5.  1.3 0.3]\n",
      " [4.5 1.3 0.3]\n",
      " [4.4 1.3 0.2]\n",
      " [5.  1.6 0.6]\n",
      " [5.1 1.9 0.4]\n",
      " [4.8 1.4 0.3]\n",
      " [5.1 1.6 0.2]\n",
      " [4.6 1.4 0.2]\n",
      " [5.3 1.5 0.2]\n",
      " [5.  1.4 0.2]\n",
      " [7.  4.7 1.4]\n",
      " [6.4 4.5 1.5]\n",
      " [6.9 4.9 1.5]\n",
      " [5.5 4.  1.3]\n",
      " [6.5 4.6 1.5]\n",
      " [5.7 4.5 1.3]\n",
      " [6.3 4.7 1.6]\n",
      " [4.9 3.3 1. ]\n",
      " [6.6 4.6 1.3]\n",
      " [5.2 3.9 1.4]\n",
      " [5.  3.5 1. ]\n",
      " [5.9 4.2 1.5]\n",
      " [6.  4.  1. ]\n",
      " [6.1 4.7 1.4]\n",
      " [5.6 3.6 1.3]\n",
      " [6.7 4.4 1.4]\n",
      " [5.6 4.5 1.5]\n",
      " [5.8 4.1 1. ]\n",
      " [6.2 4.5 1.5]\n",
      " [5.6 3.9 1.1]\n",
      " [5.9 4.8 1.8]\n",
      " [6.1 4.  1.3]\n",
      " [6.3 4.9 1.5]\n",
      " [6.1 4.7 1.2]\n",
      " [6.4 4.3 1.3]\n",
      " [6.6 4.4 1.4]\n",
      " [6.8 4.8 1.4]\n",
      " [6.7 5.  1.7]\n",
      " [6.  4.5 1.5]\n",
      " [5.7 3.5 1. ]\n",
      " [5.5 3.8 1.1]\n",
      " [5.5 3.7 1. ]\n",
      " [5.8 3.9 1.2]\n",
      " [6.  5.1 1.6]\n",
      " [5.4 4.5 1.5]\n",
      " [6.  4.5 1.6]\n",
      " [6.7 4.7 1.5]\n",
      " [6.3 4.4 1.3]\n",
      " [5.6 4.1 1.3]\n",
      " [5.5 4.  1.3]\n",
      " [5.5 4.4 1.2]\n",
      " [6.1 4.6 1.4]\n",
      " [5.8 4.  1.2]\n",
      " [5.  3.3 1. ]\n",
      " [5.6 4.2 1.3]\n",
      " [5.7 4.2 1.2]\n",
      " [5.7 4.2 1.3]\n",
      " [6.2 4.3 1.3]\n",
      " [5.1 3.  1.1]\n",
      " [5.7 4.1 1.3]\n",
      " [6.3 6.  2.5]\n",
      " [5.8 5.1 1.9]\n",
      " [7.1 5.9 2.1]\n",
      " [6.3 5.6 1.8]\n",
      " [6.5 5.8 2.2]\n",
      " [7.6 6.6 2.1]\n",
      " [4.9 4.5 1.7]\n",
      " [7.3 6.3 1.8]\n",
      " [6.7 5.8 1.8]\n",
      " [7.2 6.1 2.5]\n",
      " [6.5 5.1 2. ]\n",
      " [6.4 5.3 1.9]\n",
      " [6.8 5.5 2.1]\n",
      " [5.7 5.  2. ]\n",
      " [5.8 5.1 2.4]\n",
      " [6.4 5.3 2.3]\n",
      " [6.5 5.5 1.8]\n",
      " [7.7 6.7 2.2]\n",
      " [7.7 6.9 2.3]\n",
      " [6.  5.  1.5]\n",
      " [6.9 5.7 2.3]\n",
      " [5.6 4.9 2. ]\n",
      " [7.7 6.7 2. ]\n",
      " [6.3 4.9 1.8]\n",
      " [6.7 5.7 2.1]\n",
      " [7.2 6.  1.8]\n",
      " [6.2 4.8 1.8]\n",
      " [6.1 4.9 1.8]\n",
      " [6.4 5.6 2.1]\n",
      " [7.2 5.8 1.6]\n",
      " [7.4 6.1 1.9]\n",
      " [7.9 6.4 2. ]\n",
      " [6.4 5.6 2.2]\n",
      " [6.3 5.1 1.5]\n",
      " [6.1 5.6 1.4]\n",
      " [7.7 6.1 2.3]\n",
      " [6.3 5.6 2.4]\n",
      " [6.4 5.5 1.8]\n",
      " [6.  4.8 1.8]\n",
      " [6.9 5.4 2.1]\n",
      " [6.7 5.6 2.4]\n",
      " [6.9 5.1 2.3]\n",
      " [5.8 5.1 1.9]\n",
      " [6.8 5.9 2.3]\n",
      " [6.7 5.7 2.5]\n",
      " [6.7 5.2 2.3]\n",
      " [6.3 5.  1.9]\n",
      " [6.5 5.2 2. ]\n",
      " [6.2 5.4 2.3]\n",
      " [5.9 5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "filter = select.get_support()\n",
    "features = array(iris.feature_names)\n",
    " \n",
    "print(\"All features:\")\n",
    "print(features)\n",
    " \n",
    "print(\"Selected best 3:\")\n",
    "print(features[filter])\n",
    "print(z) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3bfdf",
   "metadata": {},
   "source": [
    "## Select K best for Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41d08b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature data dimension:  (506, 13)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "\n",
    "print(\"Feature data dimension: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1eb520",
   "metadata": {},
   "source": [
    "Again, we'll define the model by using SelectKBest class. For regression, we'll set 'f_regression'  method as a scoring function. The target number of features to select is 8. We'll fit and transform the model on training x and y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d5e1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After selecting best 8 features: (506, 8)\n"
     ]
    }
   ],
   "source": [
    "select = SelectKBest(score_func=f_regression, k=8)\n",
    "z = select.fit_transform(x, y) \n",
    " \n",
    "print(\"After selecting best 8 features:\", z.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2a082",
   "metadata": {},
   "source": [
    "To identify the selected features we can use get_support() function and filter out them from the features list. The z object contains selected x data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47a43e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features:\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Selected best 8:\n",
      "['CRIM' 'INDUS' 'NOX' 'RM' 'RAD' 'TAX' 'PTRATIO' 'LSTAT']\n",
      "[[6.320e-03 2.310e+00 5.380e-01 ... 2.960e+02 1.530e+01 4.980e+00]\n",
      " [2.731e-02 7.070e+00 4.690e-01 ... 2.420e+02 1.780e+01 9.140e+00]\n",
      " [2.729e-02 7.070e+00 4.690e-01 ... 2.420e+02 1.780e+01 4.030e+00]\n",
      " ...\n",
      " [6.076e-02 1.193e+01 5.730e-01 ... 2.730e+02 2.100e+01 5.640e+00]\n",
      " [1.096e-01 1.193e+01 5.730e-01 ... 2.730e+02 2.100e+01 6.480e+00]\n",
      " [4.741e-02 1.193e+01 5.730e-01 ... 2.730e+02 2.100e+01 7.880e+00]]\n"
     ]
    }
   ],
   "source": [
    "filter = select.get_support()\n",
    "features = array(boston.feature_names)\n",
    " \n",
    "print(\"All features:\")\n",
    "print(features)\n",
    " \n",
    "print(\"Selected best 8:\")\n",
    "print(features[filter])\n",
    "print(z) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60e22b",
   "metadata": {},
   "source": [
    "## 2. Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e41c3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2287118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature data dimension:  (506, 13)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "\n",
    "print(\"Feature data dimension: \", x.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "146ee06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "selector = RFE(estimator, n_features_to_select=8, step=1)\n",
    "selector = selector.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66b8c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask data:  [ True False False False  True  True False  True  True  True  True False\n",
      "  True]\n",
      "Ranking:  [1 5 3 6 1 1 4 1 1 1 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "filter = selector.support_\n",
    "ranking = selector.ranking_\n",
    "\n",
    "print(\"Mask data: \", filter)\n",
    "print(\"Ranking: \", ranking) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "114083ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features:\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "Selected features:\n",
      "['CRIM' 'NOX' 'RM' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "features = array(boston.feature_names)\n",
    "print(\"All features:\")\n",
    "print(features)\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(features[filter])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec65fa4",
   "metadata": {},
   "source": [
    "## EXTRA : PCA as Feature Selection/ Speed up Factor etc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b4bc6",
   "metadata": {},
   "source": [
    "https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
