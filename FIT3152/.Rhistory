result_bagging
# Evaluating the model accuracy
bagging_accuracy = (result_bagging[2, 2] + result_bagging[1, 1]) / (result_bagging[2, 2] + result_bagging[1, 1] + result_bagging[2, 1] + result_bagging[1, 2])
bagging_accuracy
# Predict using the testing dataset after training the model
predict_boosting = predict(boosting_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Boosting Confusion Matrix
result_boosting = predict_boosting$confusion
colnames(result_boosting) = c("legitimate", "phishing")
rownames(result_boosting) = c("legitimate", "phishing")
result_boosting
# Evaluating the model accuracy
boosting_accuracy = (result_boosting[2, 2] + result_boosting[1, 1]) / (result_boosting[2, 2] + result_boosting[1, 1] + result_boosting[2, 1] + result_boosting[1, 2])
boosting_accuracy
# Predict using the testing dataset after training the model
predict_random_forest = predict(random_forest_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Random Forest Confusion Matrix
result_random_forest = table(actual = PD_na_free.test$Class, predicted = predict_random_forest)
colnames(result_random_forest) = c("legitimate", "phishing")
rownames(result_random_forest) = c("legitimate", "phishing")
result_random_forest
# Evaluating the model accuracy
random_forest_accuracy = (result_random_forest[2, 2] + result_random_forest[1, 1]) / (result_random_forest[2, 2] + result_random_forest[1, 1] + result_random_forest[2, 1] + result_random_forest[1, 2])
random_forest_accuracy
confidence_decision_tree = predict(decision_tree_model, newdata = PD_na_free.test, type = "vector")
prediction_decision_tree = prediction(confidence_decision_tree[, 2], PD_na_free.test$Class)
prediction_decision_tree
performance_decision_tree = performance(prediction_decision_tree, "tpr", "fpr")
performance_decision_tree
confidence_naive_bayes = predict(naive_bayes_model, newdata = PD_na_free.test, type = "raw")
prediction_naive_bayes = prediction(confidence_naive_bayes[, 2], PD_na_free.test$Class)
performance_naive_bayes = performance(prediction_naive_bayes, "tpr", "fpr")
confidence_bagging = predict(bagging_model, newdata = PD_na_free.test, type = "prob")
confidence_bagging = confidence_bagging$prob
confidence_bagging = confidence_bagging[, 1:2]
prediction_bagging = prediction(confidence_bagging[, 2], PD_na_free.test$Class)
performance_bagging = performance(prediction_bagging, "tpr", "fpr")
confidence_boosting = predict(boosting_model, newdata = PD_na_free.test, type = "prob")
confidence_boosting = confidence_boosting$prob
prediction_boosting = prediction(confidence_boosting[, 2], PD_na_free.test$Class)
performance_boosting = performance(prediction_boosting, "tpr", "fpr")
confidence_random_forest = predict(random_forest_model, newdata = PD_na_free.test, type = "prob")
prediction_random_forest = prediction(confidence_random_forest[, 2], PD_na_free.test$Class)
performance_random_forest = performance(prediction_random_forest,"tpr","fpr")
plot(performance_decision_tree, col = "royalblue", main = "ROC Curve For 5 Models")
plot(performance_naive_bayes, col = "slategray2", add = TRUE)
plot(performance_bagging, col = "darkseagreen3", add = TRUE)
plot(performance_boosting, col = "thistle", add = TRUE)
plot(performance_random_forest, col = "rosybrown1", add = TRUE)
abline(0, 1)
legend("topleft", legend = c("Decision Tree", "Na誰ve-Bayes", "Bagging", "Boosting", "Random Forest"), fill = c("royalblue", "slategray2", "darkseagreen3", "thistle", "rosybrown1"))
auc_decision_tree = performance(prediction_decision_tree, "auc")
auc_decision_tree_num = as.numeric(auc_decision_tree@y.values)
auc_decision_tree_num
auc_naive_bayes = performance(prediction_naive_bayes, "auc")
auc_naive_bayes_num = as.numeric(auc_naive_bayes@y.values)
auc_naive_bayes_num
auc_bagging = performance(prediction_bagging, "auc")
auc_bagging_num = as.numeric(auc_bagging@y.values)
auc_bagging_num
auc_boosting = performance(prediction_boosting, "auc")
auc_boosting_num = as.numeric(auc_boosting@y.values)
auc_boosting_num
auc_random_forest = performance(prediction_random_forest, "auc")
auc_random_forest_num = as.numeric(auc_random_forest@y.values)
auc_random_forest_num
decision_tree_accuracy_q7 = performance(prediction_decision_tree, "acc")
decision_tree_accuracy_q7_num = as.numeric(max(decision_tree_accuracy_q7@y.values[[1]]))
decision_tree_accuracy_q7_num
naive_bayes_accuracy_q7 = performance(prediction_naive_bayes, "acc")
naive_bayes_accuracy_q7_num = as.numeric(max(naive_bayes_accuracy_q7@y.values[[1]]))
naive_bayes_accuracy_q7_num
bagging_accuracy_q7 = performance(prediction_bagging, "acc")
bagging_accuracy_q7_num = as.numeric(max(bagging_accuracy_q7@y.values[[1]]))
bagging_accuracy_q7_num
boosting_accuracy_q7 = performance(prediction_boosting, "acc")
boosting_accuracy_q7_num = as.numeric(max(boosting_accuracy_q7@y.values[[1]]))
boosting_accuracy_q7_num
random_forest_accuracy_q7 = performance(prediction_random_forest, "acc")
random_forest_accuracy_q7_num = as.numeric(max(random_forest_accuracy_q7@y.values[[1]]))
random_forest_accuracy_q7_num
q5_model_accuracy = c(decision_tree_accuracy, naive_bayes_accuracy, bagging_accuracy, boosting_accuracy, random_forest_accuracy)
q6_model_accuracy = c(decision_tree_accuracy_q7_num, naive_bayes_accuracy_q7_num, bagging_accuracy_q7_num, boosting_accuracy_q7_num, random_forest_accuracy_q7_num)
average_model_accuracy = c((decision_tree_accuracy + decision_tree_accuracy_q7_num) / 2, (naive_bayes_accuracy + naive_bayes_accuracy_q7_num) / 2, (bagging_accuracy + bagging_accuracy_q7_num) / 2, (boosting_accuracy + boosting_accuracy_q7_num) / 2, (random_forest_accuracy + random_forest_accuracy_q7_num) / 2)
comparison_tbl = data.frame(q5_model_accuracy, q6_model_accuracy, average_model_accuracy)
rownames(comparison_tbl) = c("Decision Tree", "Na誰ve Bayes", "Bagging", "Boosting", "Random Forest")
colnames(comparison_tbl) = c("Question 5 Model Accuracy", "Question 6 Model Accuracy", "Average Model Accuracy")
comparison_tbl
summary(decision_tree_model)
acc = c()
for(tab in naive_bayes_model[["tables"]]) {
acc = c(acc, ((tab[2, 2] + tab[1, 1]) / (tab[2, 2] + tab[1, 1] + tab[2, 1] + tab[1, 2])))
}
naive_bayes_model_accuracy = data.frame(Attributes = colnames(PD_na_free[, 1:25]), Accuracy = acc)
naive_bayes_model_accuracy = naive_bayes_model_accuracy[order(naive_bayes_model_accuracy$Accuracy, decreasing = TRUE), ]
naive_bayes_model_accuracy
bagging_model$importance
barplot(bagging_model$importance[order(bagging_model$importance, decreasing = TRUE)], ylim = c(0, 40), las = 2, main = "Bagging Model Important Predictors", xlab = "Model Predictors", ylab = "Accuracy", col = "lightsteelblue1")
boosting_model$importance
barplot(boosting_model$importance[order(boosting_model$importance, decreasing = TRUE)], ylim = c(0, 30), las = 2, main = "Boosting Model Important Predictors", xlab = "Model Predictors", ylab = "Accuracy", col = "cornflowerblue")
random_forest_model$importance
varImpPlot(random_forest_model, main = "Random Forest Important Predictors", bg = "cadetblue3", cex=1)
plot(performance_decision_tree, col = "royalblue", main = "ROC Curve For 5 Models", lwd = 2)
plot(performance_naive_bayes, col = "slategray2", add = TRUE)
plot(performance_bagging, col = "darkseagreen3", add = TRUE)
plot(performance_boosting, col = "thistle", add = TRUE)
plot(performance_random_forest, col = "rosybrown1", add = TRUE)
abline(0, 1)
legend("topleft", legend = c("Decision Tree", "Na誰ve-Bayes", "Bagging", "Boosting", "Random Forest"), fill = c("royalblue", "slategray2", "darkseagreen3", "thistle", "rosybrown1"))
plot(performance_decision_tree, col = "royalblue", main = "ROC Curve For 5 Models", lwd = 2)
plot(performance_naive_bayes, col = "slategray2", add = TRUE, lwd = 2)
plot(performance_bagging, col = "darkseagreen3", add = TRUE, lwd = 2)
plot(performance_boosting, col = "thistle", add = TRUE, lwd = 2)
plot(performance_random_forest, col = "rosybrown1", add = TRUE, lwd = 2)
abline(0, 1)
legend("topleft", legend = c("Decision Tree", "Na誰ve-Bayes", "Bagging", "Boosting", "Random Forest"), fill = c("royalblue", "slategray2", "darkseagreen3", "thistle", "rosybrown1"))
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Monash/FIT3152")
library(tree)
library(ROCR)
library(caret)
library(rpart)
library(dplyr)
library(tidyr)
library(e1071)
library(ipred)
library(adabag)
library(ggpubr)
library(ggplot2)
library(reshape)
library(corrplot)
library(factoextra)
library(randomForest)
rm(list = ls())
Phish <- read.csv("PhishingData.csv")
set.seed(33085625)
L <- as.data.frame(c(1:50))
L <- L[sample(nrow(L), 10, replace = FALSE),]
Phish <- Phish[(Phish$A01 %in% L),]
PD <- Phish[sample(nrow(Phish), 2000, replace = FALSE),] # sample of 2000 rows
dim(PD)
names(PD)
as.data.frame(table(PD["Class"]))
str(PD)
summary(PD)
# Compute the correlation matrix
cor_matrix <- cor(PD, use = "complete.obs")  # Omit NA values
# Create a correlation plot
corrplot(cor_matrix, method = "color", order = "AOE", tl.cex = 0.7, tl.col = "black",
title = "PD Predictors Correlation Plot", type = "upper")
PD = data.frame(PD)
PD$Class = factor(PD$Class)
PD_na_free = PD[complete.cases(PD),]
PD_na_free = na.omit(PD)
dim(PD_na_free)
str(PD_na_free)
summary(PD_na_free)
set.seed(33085625)
train.row = sample(1:nrow(PD_na_free), 0.7*nrow(PD_na_free))
PD_na_free.train = PD_na_free[train.row,]
PD_na_free.test = PD_na_free[-train.row,]
str(PD_na_free.test)
decision_tree_model = tree(Class ~., data = PD_na_free.train)
summary(decision_tree_model)
plot(decision_tree_model)
text(decision_tree_model, pretty = 0)
naive_bayes_model = naiveBayes(Class ~., data = PD_na_free.train)
summary(naive_bayes_model)
# Check for NA values because Bagging algorithm cannot process NA values directly
sum(is.na(PD_na_free.train))
bagging_model <- bagging(Class ~., data = PD_na_free.train, coob = TRUE, resampling = "bootstrap")
summary(bagging_model)
# Check for NA values because Boosting algorithm cannot process NA values directly
sum(is.na(PD_na_free.train))
boosting_model = boosting(Class ~., data = PD_na_free.train)
summary(boosting_model)
random_forest_model = randomForest(Class ~., data = PD_na_free.train)
summary(random_forest_model)
# Predict using the testing dataset after training the model
predict_decision_tree = predict(decision_tree_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Decision Tree Confusion Matrix
result_decision_tree = table(actual = PD_na_free.test$Class, prediction = predict_decision_tree)
colnames(result_decision_tree) = c("legitimate", "phishing")
rownames(result_decision_tree) = c("legitimate", "phishing")
result_decision_tree
# Evaluating the model accuracy
decision_tree_accuracy = (result_decision_tree[2, 2] + result_decision_tree[1, 1]) / (result_decision_tree[2, 2] + result_decision_tree[1, 1] + result_decision_tree[2, 1] + result_decision_tree[1, 2])
decision_tree_accuracy
# Predict using the testing dataset after training the model
predict_naive_bayes = predict(naive_bayes_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Na誰ve-Bayes Confusion Matrix
result_naive_bayes = table(actual = PD_na_free.test$Class, predicted = predict_naive_bayes)
colnames(result_naive_bayes) = c("legitimate", "phishing")
rownames(result_naive_bayes) = c("legitimate", "phishing")
result_naive_bayes
# Evaluating the model accuracy
naive_bayes_accuracy = (result_naive_bayes[2, 2] + result_naive_bayes[1, 1]) / (result_naive_bayes[2, 2] + result_naive_bayes[1, 1] + result_naive_bayes[2, 1] + result_naive_bayes[1, 2])
naive_bayes_accuracy
# Predict using the testing dataset after training the model
predict_bagging = predict(bagging_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Bagging Confusion Matrix
result_bagging = predict_bagging$confusion
colnames(result_bagging) = c("legitimate", "phishing")
rownames(result_bagging) = c("legitimate", "phishing")
result_bagging
# Evaluating the model accuracy
bagging_accuracy = (result_bagging[2, 2] + result_bagging[1, 1]) / (result_bagging[2, 2] + result_bagging[1, 1] + result_bagging[2, 1] + result_bagging[1, 2])
bagging_accuracy
# Predict using the testing dataset after training the model
predict_boosting = predict(boosting_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Boosting Confusion Matrix
result_boosting = predict_boosting$confusion
colnames(result_boosting) = c("legitimate", "phishing")
rownames(result_boosting) = c("legitimate", "phishing")
result_boosting
# Evaluating the model accuracy
boosting_accuracy = (result_boosting[2, 2] + result_boosting[1, 1]) / (result_boosting[2, 2] + result_boosting[1, 1] + result_boosting[2, 1] + result_boosting[1, 2])
boosting_accuracy
# Predict using the testing dataset after training the model
predict_random_forest = predict(random_forest_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Random Forest Confusion Matrix
result_random_forest = table(actual = PD_na_free.test$Class, predicted = predict_random_forest)
colnames(result_random_forest) = c("legitimate", "phishing")
rownames(result_random_forest) = c("legitimate", "phishing")
result_random_forest
# Evaluating the model accuracy
random_forest_accuracy = (result_random_forest[2, 2] + result_random_forest[1, 1]) / (result_random_forest[2, 2] + result_random_forest[1, 1] + result_random_forest[2, 1] + result_random_forest[1, 2])
random_forest_accuracy
confidence_decision_tree = predict(decision_tree_model, newdata = PD_na_free.test, type = "vector")
prediction_decision_tree = prediction(confidence_decision_tree[, 2], PD_na_free.test$Class)
prediction_decision_tree
performance_decision_tree = performance(prediction_decision_tree, "tpr", "fpr")
performance_decision_tree
confidence_naive_bayes = predict(naive_bayes_model, newdata = PD_na_free.test, type = "raw")
prediction_naive_bayes = prediction(confidence_naive_bayes[, 2], PD_na_free.test$Class)
performance_naive_bayes = performance(prediction_naive_bayes, "tpr", "fpr")
confidence_bagging = predict(bagging_model, newdata = PD_na_free.test, type = "prob")
confidence_bagging = confidence_bagging$prob
confidence_bagging = confidence_bagging[, 1:2]
prediction_bagging = prediction(confidence_bagging[, 2], PD_na_free.test$Class)
performance_bagging = performance(prediction_bagging, "tpr", "fpr")
confidence_boosting = predict(boosting_model, newdata = PD_na_free.test, type = "prob")
confidence_boosting = confidence_boosting$prob
prediction_boosting = prediction(confidence_boosting[, 2], PD_na_free.test$Class)
performance_boosting = performance(prediction_boosting, "tpr", "fpr")
confidence_random_forest = predict(random_forest_model, newdata = PD_na_free.test, type = "prob")
prediction_random_forest = prediction(confidence_random_forest[, 2], PD_na_free.test$Class)
performance_random_forest = performance(prediction_random_forest,"tpr","fpr")
plot(performance_decision_tree, col = "royalblue", main = "ROC Curve For 5 Models", lwd = 2)
plot(performance_naive_bayes, col = "slategray2", add = TRUE, lwd = 2)
plot(performance_bagging, col = "darkseagreen3", add = TRUE, lwd = 2)
plot(performance_boosting, col = "thistle", add = TRUE, lwd = 2)
plot(performance_random_forest, col = "rosybrown1", add = TRUE, lwd = 2)
# lwd was used to increase the thickness of the line in the plot
abline(0, 1)
legend("topleft", legend = c("Decision Tree", "Na誰ve-Bayes", "Bagging", "Boosting", "Random Forest"), fill = c("royalblue", "slategray2", "darkseagreen3", "thistle", "rosybrown1"))
auc_decision_tree = performance(prediction_decision_tree, "auc")
auc_decision_tree_num = as.numeric(auc_decision_tree@y.values)
auc_decision_tree_num
auc_naive_bayes = performance(prediction_naive_bayes, "auc")
auc_naive_bayes_num = as.numeric(auc_naive_bayes@y.values)
auc_naive_bayes_num
auc_bagging = performance(prediction_bagging, "auc")
auc_bagging_num = as.numeric(auc_bagging@y.values)
auc_bagging_num
auc_boosting = performance(prediction_boosting, "auc")
auc_boosting_num = as.numeric(auc_boosting@y.values)
auc_boosting_num
auc_random_forest = performance(prediction_random_forest, "auc")
auc_random_forest_num = as.numeric(auc_random_forest@y.values)
auc_random_forest_num
decision_tree_accuracy_q7 = performance(prediction_decision_tree, "acc")
decision_tree_accuracy_q7_num = as.numeric(max(decision_tree_accuracy_q7@y.values[[1]]))
decision_tree_accuracy_q7_num
naive_bayes_accuracy_q7 = performance(prediction_naive_bayes, "acc")
naive_bayes_accuracy_q7_num = as.numeric(max(naive_bayes_accuracy_q7@y.values[[1]]))
naive_bayes_accuracy_q7_num
bagging_accuracy_q7 = performance(prediction_bagging, "acc")
bagging_accuracy_q7_num = as.numeric(max(bagging_accuracy_q7@y.values[[1]]))
bagging_accuracy_q7_num
boosting_accuracy_q7 = performance(prediction_boosting, "acc")
boosting_accuracy_q7_num = as.numeric(max(boosting_accuracy_q7@y.values[[1]]))
boosting_accuracy_q7_num
random_forest_accuracy_q7 = performance(prediction_random_forest, "acc")
random_forest_accuracy_q7_num = as.numeric(max(random_forest_accuracy_q7@y.values[[1]]))
random_forest_accuracy_q7_num
q5_model_accuracy = c(decision_tree_accuracy, naive_bayes_accuracy, bagging_accuracy, boosting_accuracy, random_forest_accuracy)
q6_model_accuracy = c(decision_tree_accuracy_q7_num, naive_bayes_accuracy_q7_num, bagging_accuracy_q7_num, boosting_accuracy_q7_num, random_forest_accuracy_q7_num)
average_model_accuracy = c((decision_tree_accuracy + decision_tree_accuracy_q7_num) / 2, (naive_bayes_accuracy + naive_bayes_accuracy_q7_num) / 2, (bagging_accuracy + bagging_accuracy_q7_num) / 2, (boosting_accuracy + boosting_accuracy_q7_num) / 2, (random_forest_accuracy + random_forest_accuracy_q7_num) / 2)
comparison_tbl = data.frame(q5_model_accuracy, q6_model_accuracy, average_model_accuracy)
rownames(comparison_tbl) = c("Decision Tree", "Na誰ve Bayes", "Bagging", "Boosting", "Random Forest")
colnames(comparison_tbl) = c("Question 5 Model Accuracy", "Question 6 Model Accuracy", "Average Model Accuracy")
comparison_tbl
summary(decision_tree_model)
acc = c()
for(tab in naive_bayes_model[["tables"]]) {
acc = c(acc, ((tab[2, 2] + tab[1, 1]) / (tab[2, 2] + tab[1, 1] + tab[2, 1] + tab[1, 2])))
}
naive_bayes_model_accuracy = data.frame(Attributes = colnames(PD_na_free[, 1:25]), Accuracy = acc)
naive_bayes_model_accuracy = naive_bayes_model_accuracy[order(naive_bayes_model_accuracy$Accuracy, decreasing = TRUE), ]
naive_bayes_model_accuracy
bagging_model$importance
barplot(bagging_model$importance[order(bagging_model$importance, decreasing = TRUE)], ylim = c(0, 40), las = 2, main = "Bagging Model Important Predictors", xlab = "Model Predictors", ylab = "Accuracy", col = "lightsteelblue1")
boosting_model$importance
barplot(boosting_model$importance[order(boosting_model$importance, decreasing = TRUE)], ylim = c(0, 30), las = 2, main = "Boosting Model Important Predictors", xlab = "Model Predictors", ylab = "Accuracy", col = "cornflowerblue")
random_forest_model$importance
varImpPlot(random_forest_model, main = "Random Forest Important Predictors", bg = "cadetblue3", cex=1)
plot(performance_decision_tree, col = "royalblue", main = "ROC Curve For 5 Models", lwd = 2)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Monash/FIT3152")
library(tree)
library(ROCR)
library(caret)
library(rpart)
library(dplyr)
library(tidyr)
library(e1071)
library(ipred)
library(adabag)
library(ggpubr)
library(ggplot2)
library(reshape)
library(corrplot)
library(factoextra)
library(randomForest)
rm(list = ls())
Phish <- read.csv("PhishingData.csv")
set.seed(33085625)
L <- as.data.frame(c(1:50))
L <- L[sample(nrow(L), 10, replace = FALSE),]
Phish <- Phish[(Phish$A01 %in% L),]
PD <- Phish[sample(nrow(Phish), 2000, replace = FALSE),] # sample of 2000 rows
dim(PD)
names(PD)
as.data.frame(table(PD["Class"]))
str(PD)
summary(PD)
# Compute the correlation matrix
cor_matrix <- cor(PD, use = "complete.obs")  # Omit NA values
# Create a correlation plot
corrplot(cor_matrix, method = "color", order = "AOE", tl.cex = 0.7, tl.col = "black",
title = "PD Predictors Correlation Plot", type = "upper")
PD = data.frame(PD)
PD$Class = factor(PD$Class)
PD_na_free = PD[complete.cases(PD),]
PD_na_free = na.omit(PD)
dim(PD_na_free)
str(PD_na_free) # Confirm after removal of NA values, the predictor 'Class' is still a factor
summary(PD_na_free)
set.seed(33085625)
train.row = sample(1:nrow(PD_na_free), 0.7*nrow(PD_na_free))
PD_na_free.train = PD_na_free[train.row,]
PD_na_free.test = PD_na_free[-train.row,]
decision_tree_model = tree(Class ~., data = PD_na_free.train)
summary(decision_tree_model)
plot(decision_tree_model)
text(decision_tree_model, pretty = 0)
naive_bayes_model = naiveBayes(Class ~., data = PD_na_free.train)
summary(naive_bayes_model)
# Check for NA values because Bagging algorithm cannot process NA values directly
sum(is.na(PD_na_free.train))
bagging_model <- bagging(Class ~., data = PD_na_free.train, coob = TRUE, resampling = "bootstrap")
summary(bagging_model)
# Check for NA values because Boosting algorithm cannot process NA values directly
sum(is.na(PD_na_free.train))
boosting_model = boosting(Class ~., data = PD_na_free.train)
summary(boosting_model)
random_forest_model = randomForest(Class ~., data = PD_na_free.train)
summary(random_forest_model)
# Predict using the testing dataset after training the model
predict_decision_tree = predict(decision_tree_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Decision Tree Confusion Matrix
result_decision_tree = table(actual = PD_na_free.test$Class, prediction = predict_decision_tree)
colnames(result_decision_tree) = c("legitimate", "phishing")
rownames(result_decision_tree) = c("legitimate", "phishing")
result_decision_tree
# Evaluating the model accuracy
decision_tree_accuracy = (result_decision_tree[2, 2] + result_decision_tree[1, 1]) / (result_decision_tree[2, 2] + result_decision_tree[1, 1] + result_decision_tree[2, 1] + result_decision_tree[1, 2])
decision_tree_accuracy
# Predict using the testing dataset after training the model
predict_naive_bayes = predict(naive_bayes_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Na誰ve-Bayes Confusion Matrix
result_naive_bayes = table(actual = PD_na_free.test$Class, predicted = predict_naive_bayes)
colnames(result_naive_bayes) = c("legitimate", "phishing")
rownames(result_naive_bayes) = c("legitimate", "phishing")
result_naive_bayes
# Evaluating the model accuracy
naive_bayes_accuracy = (result_naive_bayes[2, 2] + result_naive_bayes[1, 1]) / (result_naive_bayes[2, 2] + result_naive_bayes[1, 1] + result_naive_bayes[2, 1] + result_naive_bayes[1, 2])
naive_bayes_accuracy
# Predict using the testing dataset after training the model
predict_bagging = predict(bagging_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Bagging Confusion Matrix
result_bagging = predict_bagging$confusion
colnames(result_bagging) = c("legitimate", "phishing")
rownames(result_bagging) = c("legitimate", "phishing")
result_bagging
# Evaluating the model accuracy
bagging_accuracy = (result_bagging[2, 2] + result_bagging[1, 1]) / (result_bagging[2, 2] + result_bagging[1, 1] + result_bagging[2, 1] + result_bagging[1, 2])
bagging_accuracy
# Predict using the testing dataset after training the model
predict_boosting = predict(boosting_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Boosting Confusion Matrix
result_boosting = predict_boosting$confusion
colnames(result_boosting) = c("legitimate", "phishing")
rownames(result_boosting) = c("legitimate", "phishing")
result_boosting
# Evaluating the model accuracy
boosting_accuracy = (result_boosting[2, 2] + result_boosting[1, 1]) / (result_boosting[2, 2] + result_boosting[1, 1] + result_boosting[2, 1] + result_boosting[1, 2])
boosting_accuracy
# Predict using the testing dataset after training the model
predict_random_forest = predict(random_forest_model, newdata = PD_na_free.test, type = "class")
# Evaluating the model performance
# Random Forest Confusion Matrix
result_random_forest = table(actual = PD_na_free.test$Class, predicted = predict_random_forest)
colnames(result_random_forest) = c("legitimate", "phishing")
rownames(result_random_forest) = c("legitimate", "phishing")
result_random_forest
# Evaluating the model accuracy
random_forest_accuracy = (result_random_forest[2, 2] + result_random_forest[1, 1]) / (result_random_forest[2, 2] + result_random_forest[1, 1] + result_random_forest[2, 1] + result_random_forest[1, 2])
random_forest_accuracy
confidence_decision_tree = predict(decision_tree_model, newdata = PD_na_free.test, type = "vector")
prediction_decision_tree = prediction(confidence_decision_tree[, 2], PD_na_free.test$Class)
prediction_decision_tree
performance_decision_tree = performance(prediction_decision_tree, "tpr", "fpr")
performance_decision_tree
confidence_naive_bayes = predict(naive_bayes_model, newdata = PD_na_free.test, type = "raw")
prediction_naive_bayes = prediction(confidence_naive_bayes[, 2], PD_na_free.test$Class)
performance_naive_bayes = performance(prediction_naive_bayes, "tpr", "fpr")
confidence_bagging = predict(bagging_model, newdata = PD_na_free.test, type = "prob")
confidence_bagging = confidence_bagging$prob
confidence_bagging = confidence_bagging[, 1:2]
prediction_bagging = prediction(confidence_bagging[, 2], PD_na_free.test$Class)
performance_bagging = performance(prediction_bagging, "tpr", "fpr")
confidence_boosting = predict(boosting_model, newdata = PD_na_free.test, type = "prob")
confidence_boosting = confidence_boosting$prob
prediction_boosting = prediction(confidence_boosting[, 2], PD_na_free.test$Class)
performance_boosting = performance(prediction_boosting, "tpr", "fpr")
confidence_random_forest = predict(random_forest_model, newdata = PD_na_free.test, type = "prob")
prediction_random_forest = prediction(confidence_random_forest[, 2], PD_na_free.test$Class)
performance_random_forest = performance(prediction_random_forest,"tpr","fpr")
plot(performance_decision_tree, col = "royalblue", main = "ROC Curve For 5 Models", lwd = 2)
plot(performance_naive_bayes, col = "slategray2", add = TRUE, lwd = 2)
plot(performance_bagging, col = "darkseagreen3", add = TRUE, lwd = 2)
plot(performance_boosting, col = "thistle", add = TRUE, lwd = 2)
plot(performance_random_forest, col = "rosybrown1", add = TRUE, lwd = 2)
# lwd was used to increase the thickness of the line in the plot
abline(0, 1)
legend("topleft", legend = c("Decision Tree", "Na誰ve-Bayes", "Bagging", "Boosting", "Random Forest"), fill = c("royalblue", "slategray2", "darkseagreen3", "thistle", "rosybrown1"))
auc_decision_tree = performance(prediction_decision_tree, "auc")
auc_decision_tree_num = as.numeric(auc_decision_tree@y.values)
auc_decision_tree_num
auc_naive_bayes = performance(prediction_naive_bayes, "auc")
auc_naive_bayes_num = as.numeric(auc_naive_bayes@y.values)
auc_naive_bayes_num
auc_bagging = performance(prediction_bagging, "auc")
auc_bagging_num = as.numeric(auc_bagging@y.values)
auc_bagging_num
auc_boosting = performance(prediction_boosting, "auc")
auc_boosting_num = as.numeric(auc_boosting@y.values)
auc_boosting_num
auc_random_forest = performance(prediction_random_forest, "auc")
auc_random_forest_num = as.numeric(auc_random_forest@y.values)
auc_random_forest_num
decision_tree_accuracy_q7 = performance(prediction_decision_tree, "acc")
decision_tree_accuracy_q7_num = as.numeric(max(decision_tree_accuracy_q7@y.values[[1]]))
decision_tree_accuracy_q7_num
naive_bayes_accuracy_q7 = performance(prediction_naive_bayes, "acc")
naive_bayes_accuracy_q7_num = as.numeric(max(naive_bayes_accuracy_q7@y.values[[1]]))
naive_bayes_accuracy_q7_num
bagging_accuracy_q7 = performance(prediction_bagging, "acc")
bagging_accuracy_q7_num = as.numeric(max(bagging_accuracy_q7@y.values[[1]]))
bagging_accuracy_q7_num
boosting_accuracy_q7 = performance(prediction_boosting, "acc")
boosting_accuracy_q7_num = as.numeric(max(boosting_accuracy_q7@y.values[[1]]))
boosting_accuracy_q7_num
random_forest_accuracy_q7 = performance(prediction_random_forest, "acc")
random_forest_accuracy_q7_num = as.numeric(max(random_forest_accuracy_q7@y.values[[1]]))
random_forest_accuracy_q7_num
q5_model_accuracy = c(decision_tree_accuracy, naive_bayes_accuracy, bagging_accuracy, boosting_accuracy, random_forest_accuracy)
q6_model_accuracy = c(decision_tree_accuracy_q7_num, naive_bayes_accuracy_q7_num, bagging_accuracy_q7_num, boosting_accuracy_q7_num, random_forest_accuracy_q7_num)
average_model_accuracy = c((decision_tree_accuracy + decision_tree_accuracy_q7_num) / 2, (naive_bayes_accuracy + naive_bayes_accuracy_q7_num) / 2, (bagging_accuracy + bagging_accuracy_q7_num) / 2, (boosting_accuracy + boosting_accuracy_q7_num) / 2, (random_forest_accuracy + random_forest_accuracy_q7_num) / 2)
comparison_tbl = data.frame(q5_model_accuracy, q6_model_accuracy, average_model_accuracy)
rownames(comparison_tbl) = c("Decision Tree", "Na誰ve Bayes", "Bagging", "Boosting", "Random Forest")
colnames(comparison_tbl) = c("Question 5 Model Accuracy", "Question 6 Model Accuracy", "Average Model Accuracy")
comparison_tbl
summary(decision_tree_model)
acc = c()
for(tab in naive_bayes_model[["tables"]]) {
acc = c(acc, ((tab[2, 2] + tab[1, 1]) / (tab[2, 2] + tab[1, 1] + tab[2, 1] + tab[1, 2])))
}
naive_bayes_model_accuracy = data.frame(Attributes = colnames(PD_na_free[, 1:25]), Accuracy = acc)
naive_bayes_model_accuracy = naive_bayes_model_accuracy[order(naive_bayes_model_accuracy$Accuracy, decreasing = TRUE), ]
naive_bayes_model_accuracy
bagging_model$importance
barplot(bagging_model$importance[order(bagging_model$importance, decreasing = TRUE)], ylim = c(0, 40), las = 2, main = "Bagging Model Important Predictors", xlab = "Model Predictors", ylab = "Accuracy", col = "lightsteelblue1")
boosting_model$importance
barplot(boosting_model$importance[order(boosting_model$importance, decreasing = TRUE)], ylim = c(0, 30), las = 2, main = "Boosting Model Important Predictors", xlab = "Model Predictors", ylab = "Accuracy", col = "cornflowerblue")
random_forest_model$importance
varImpPlot(random_forest_model, main = "Random Forest Important Predictors", bg = "cadetblue3", cex=1)
